{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea0f2a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import json\n",
    "from IPython.display import display\n",
    "\n",
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0447e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue_conversation took 3.780251 seconds to execute.\n",
      "Response generated by LLM: Machine Learning is a subset of artificial intelligence that involves the creation and use of algorithms that allow computers to learn and make decisions from data without being explicitly programmed.\n"
     ]
    }
   ],
   "source": [
    "# Define a function to continue the conversation\n",
    "import time\n",
    "import re\n",
    "\n",
    "def time_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.6f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@time_decorator\n",
    "def continue_conversation(prompt):\n",
    "    response = openai.ChatCompletion.create( model=\"gpt-4\", top_p=0,\n",
    "                                        messages=prompt,\n",
    "                                        temperature=0 )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Initial user prompt and system prompt\n",
    "\n",
    "\n",
    "initial_user_prompt=\" Define Machine Learning in short\"\n",
    "new_system_prompt =\"Answer the question asked by the user\" \n",
    "\n",
    "    \n",
    "    \n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": new_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": initial_user_prompt}]\n",
    "\n",
    "response_by_LLM = continue_conversation(history)\n",
    "print(\"Response generated by LLM:\", response_by_LLM)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc25164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: supabase in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (2.0.3)\n",
      "Requirement already satisfied: httpx<0.25.0,>=0.24.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from supabase) (0.24.1)\n",
      "Requirement already satisfied: supafunc<0.4.0,>=0.3.1 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from supabase) (0.3.1)\n",
      "Requirement already satisfied: storage3<0.7.0,>=0.5.3 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from supabase) (0.6.1)\n",
      "Requirement already satisfied: gotrue<2.0.0,>=1.3.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from supabase) (1.3.0)\n",
      "Requirement already satisfied: realtime<2.0.0,>=1.0.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from supabase) (1.0.0)\n",
      "Requirement already satisfied: postgrest<0.14.0,>=0.10.8 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from supabase) (0.13.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.10 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from gotrue<2.0.0,>=1.3.0->supabase) (2.0.2)\n",
      "Requirement already satisfied: sniffio in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx<0.25.0,>=0.24.0->supabase) (1.3.0)\n",
      "Requirement already satisfied: certifi in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx<0.25.0,>=0.24.0->supabase) (2022.6.15)\n",
      "Requirement already satisfied: idna in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx<0.25.0,>=0.24.0->supabase) (2.10)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx<0.25.0,>=0.24.0->supabase) (0.17.3)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.24.0->supabase) (4.0.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.24.0->supabase) (0.14.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.24.0->supabase) (1.1.3)\n",
      "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from postgrest<0.14.0,>=0.10.8->supabase) (2.1.0)\n",
      "Requirement already satisfied: strenum<0.5.0,>=0.4.9 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from postgrest<0.14.0,>=0.10.8->supabase) (0.4.15)\n",
      "Requirement already satisfied: packaging in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from deprecation<3.0.0,>=2.1.0->postgrest<0.14.0,>=0.10.8->supabase) (23.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue<2.0.0,>=1.3.0->supabase) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.1.2 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue<2.0.0,>=1.3.0->supabase) (2.1.2)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue<2.0.0,>=1.3.0->supabase) (4.7.1)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from realtime<2.0.0,>=1.0.0->supabase) (2.8.2)\n",
      "Requirement already satisfied: websockets<11.0,>=10.3 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from realtime<2.0.0,>=1.0.0->supabase) (10.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.0.0,>=1.0.0->supabase) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d30f9e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (0.24.1)\r\n",
      "Requirement already satisfied: sniffio in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx) (1.3.0)\r\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx) (0.17.3)\r\n",
      "Requirement already satisfied: certifi in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx) (2022.6.15)\r\n",
      "Requirement already satisfied: idna in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx) (2.10)\r\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx) (4.0.0)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx) (0.14.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx) (1.1.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9060419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 13:58:08,902:INFO - HTTP Request: GET https://tmwcifbhcnkaiqsldmlz.supabase.co/rest/v1/table_recent?select=vector_data \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from supabase import create_client, Client\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Supabase details\n",
    "url: str = \"https://tmwcifbhcnkaiqsldmlz.supabase.co\"\n",
    "key: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRtd2NpZmJoY25rYWlxc2xkbWx6Iiwicm9sZSI6ImFub24iLCJpYXQiOjE2OTkwNDI0MzAsImV4cCI6MjAxNDYxODQzMH0.tyy9IRYaXs7wkztSFAPUPSKGVz7Ng20xb_QCfWrObws\"\n",
    "\n",
    "# Create a client to connect to Supabase\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# Query to get vectors from your table (modify table name and column as per your schema)\n",
    "data = supabase.table(\"table_recent\").select(\"vector_data\").execute()\n",
    "\n",
    "# Assuming vectors are stored as arrays and not serialized\n",
    "vectors = [row['vector_data'] for row in data.data]\n",
    "\n",
    "if isinstance(vectors[0], str):\n",
    "    vectors = [json.loads(v) for v in vectors]\n",
    "\n",
    "# Convert the list of vectors to a 2D numpy array\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "# Now 'vectors' contains the vectors from your table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e09c4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04616312e-02, -1.19582536e-02, -1.97615083e-02, ...,\n",
       "        -4.11469443e-03,  1.08264564e-02, -1.60051417e-02],\n",
       "       [ 6.85736611e-02, -2.09120974e-01,  9.37654078e-02, ...,\n",
       "        -5.29735863e-01,  5.10803163e-01, -1.93054885e-01],\n",
       "       [ 3.96974981e-02, -6.17106199e-01, -2.03737959e-01, ...,\n",
       "        -3.06010187e-01,  4.10984129e-01,  1.84191912e-01],\n",
       "       ...,\n",
       "       [ 1.08481646e+00, -9.70334530e-01, -2.29545355e+00, ...,\n",
       "         2.55767870e+00,  1.08251180e-02,  2.37414169e+00],\n",
       "       [-1.03697062e+00, -2.17075372e+00,  5.36548972e-01, ...,\n",
       "         2.35963655e+00, -9.86251295e-01,  2.32671213e+00],\n",
       "       [ 1.43055069e+00, -5.37183619e+00, -6.47499800e-01, ...,\n",
       "         2.63408661e+00, -1.73421216e+00,  2.27383208e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bced2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e857d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Assuming you've downloaded the required nltk packages such as 'punkt', 'stopwords'\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    # Remove non-alphabetic characters and stopwords\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9bef942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nikitayeole/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nikitayeole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources, if not already downloaded\n",
    "def download_nltk_resources():\n",
    "    nltk.download('punkt')      # For tokenization\n",
    "    nltk.download('stopwords')  # For removing stop words\n",
    "\n",
    "# Ensure NLTK resources are downloaded before proceeding\n",
    "download_nltk_resources()\n",
    "\n",
    "# ... rest of your code ...\n",
    "\n",
    "\n",
    "def extract_text_by_page(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            # Extract text from page and preprocess\n",
    "            text = page.extract_text()\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            # Preprocess each sentence and combine into one list of words for the page\n",
    "            words = [preprocess_sentence(sentence) for sentence in sentences]\n",
    "            words = [word for sublist in words for word in sublist]  # Flatten the list\n",
    "            yield TaggedDocument(words=words, tags=[str(i)])\n",
    "pdf_path = 'AI-book.pdf'\n",
    "tagged_data = list(extract_text_by_page(pdf_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0686642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 14:04:08,137:INFO - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d50,n5,w5,mc5,s0.001,t4>', 'datetime': '2023-11-21T14:04:08.137195', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2023-11-21 14:04:08,139:INFO - collecting all words and their counts\n",
      "2023-11-21 14:04:08,140:INFO - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2023-11-21 14:04:08,212:INFO - collected 28892 word types and 1153 unique tags from a corpus of 1153 examples and 281697 words\n",
      "2023-11-21 14:04:08,212:INFO - Creating a fresh vocabulary\n",
      "2023-11-21 14:04:08,225:INFO - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 6246 unique words (21.62% of original 28892, drops 22646)', 'datetime': '2023-11-21T14:04:08.225110', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-21 14:04:08,225:INFO - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 248011 word corpus (88.04% of original 281697, drops 33686)', 'datetime': '2023-11-21T14:04:08.225458', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-21 14:04:08,236:INFO - deleting the raw counts dictionary of 28892 items\n",
      "2023-11-21 14:04:08,237:INFO - sample=0.001 downsamples 32 most-common words\n",
      "2023-11-21 14:04:08,238:INFO - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 238749.98766387967 word corpus (96.3%% of prior 248011)', 'datetime': '2023-11-21T14:04:08.238036', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-21 14:04:08,258:INFO - estimated required memory for 6246 words and 50 dimensions: 6082600 bytes\n",
      "2023-11-21 14:04:08,258:INFO - resetting layer weights\n",
      "2023-11-21 14:04:08,265:INFO - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6246 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-11-21T14:04:08.265036', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-11-21 14:04:08,340:INFO - EPOCH 0: training on 281697 raw words (239850 effective words) took 0.1s, 3391630 effective words/s\n",
      "2023-11-21 14:04:08,407:INFO - EPOCH 1: training on 281697 raw words (239995 effective words) took 0.1s, 3655041 effective words/s\n",
      "2023-11-21 14:04:08,470:INFO - EPOCH 2: training on 281697 raw words (239928 effective words) took 0.1s, 3899915 effective words/s\n",
      "2023-11-21 14:04:08,532:INFO - EPOCH 3: training on 281697 raw words (239839 effective words) took 0.1s, 3891479 effective words/s\n",
      "2023-11-21 14:04:08,602:INFO - EPOCH 4: training on 281697 raw words (239755 effective words) took 0.1s, 3511320 effective words/s\n",
      "2023-11-21 14:04:08,669:INFO - EPOCH 5: training on 281697 raw words (239995 effective words) took 0.1s, 3622498 effective words/s\n",
      "2023-11-21 14:04:08,738:INFO - EPOCH 6: training on 281697 raw words (239853 effective words) took 0.1s, 3491551 effective words/s\n",
      "2023-11-21 14:04:08,805:INFO - EPOCH 7: training on 281697 raw words (240049 effective words) took 0.1s, 3664183 effective words/s\n",
      "2023-11-21 14:04:08,874:INFO - EPOCH 8: training on 281697 raw words (239825 effective words) took 0.1s, 3516992 effective words/s\n",
      "2023-11-21 14:04:08,941:INFO - EPOCH 9: training on 281697 raw words (240015 effective words) took 0.1s, 3623666 effective words/s\n",
      "2023-11-21 14:04:09,010:INFO - EPOCH 10: training on 281697 raw words (239887 effective words) took 0.1s, 3507650 effective words/s\n",
      "2023-11-21 14:04:09,076:INFO - EPOCH 11: training on 281697 raw words (239887 effective words) took 0.1s, 3705007 effective words/s\n",
      "2023-11-21 14:04:09,146:INFO - EPOCH 12: training on 281697 raw words (239971 effective words) took 0.1s, 3469648 effective words/s\n",
      "2023-11-21 14:04:09,213:INFO - EPOCH 13: training on 281697 raw words (240069 effective words) took 0.1s, 3648853 effective words/s\n",
      "2023-11-21 14:04:09,283:INFO - EPOCH 14: training on 281697 raw words (239928 effective words) took 0.1s, 3473258 effective words/s\n",
      "2023-11-21 14:04:09,350:INFO - EPOCH 15: training on 281697 raw words (239932 effective words) took 0.1s, 3628166 effective words/s\n",
      "2023-11-21 14:04:09,420:INFO - EPOCH 16: training on 281697 raw words (239961 effective words) took 0.1s, 3464061 effective words/s\n",
      "2023-11-21 14:04:09,487:INFO - EPOCH 17: training on 281697 raw words (239836 effective words) took 0.1s, 3655342 effective words/s\n",
      "2023-11-21 14:04:09,556:INFO - EPOCH 18: training on 281697 raw words (239949 effective words) took 0.1s, 3540568 effective words/s\n",
      "2023-11-21 14:04:09,622:INFO - EPOCH 19: training on 281697 raw words (240006 effective words) took 0.1s, 3647090 effective words/s\n",
      "2023-11-21 14:04:09,692:INFO - EPOCH 20: training on 281697 raw words (239949 effective words) took 0.1s, 3527799 effective words/s\n",
      "2023-11-21 14:04:09,759:INFO - EPOCH 21: training on 281697 raw words (239867 effective words) took 0.1s, 3649513 effective words/s\n",
      "2023-11-21 14:04:09,828:INFO - EPOCH 22: training on 281697 raw words (240019 effective words) took 0.1s, 3512756 effective words/s\n",
      "2023-11-21 14:04:09,895:INFO - EPOCH 23: training on 281697 raw words (239976 effective words) took 0.1s, 3652554 effective words/s\n",
      "2023-11-21 14:04:09,964:INFO - EPOCH 24: training on 281697 raw words (239810 effective words) took 0.1s, 3532180 effective words/s\n",
      "2023-11-21 14:04:10,031:INFO - EPOCH 25: training on 281697 raw words (239940 effective words) took 0.1s, 3622798 effective words/s\n",
      "2023-11-21 14:04:10,101:INFO - EPOCH 26: training on 281697 raw words (239951 effective words) took 0.1s, 3512236 effective words/s\n",
      "2023-11-21 14:04:10,168:INFO - EPOCH 27: training on 281697 raw words (239901 effective words) took 0.1s, 3596153 effective words/s\n",
      "2023-11-21 14:04:10,237:INFO - EPOCH 28: training on 281697 raw words (239883 effective words) took 0.1s, 3522699 effective words/s\n",
      "2023-11-21 14:04:10,304:INFO - EPOCH 29: training on 281697 raw words (239867 effective words) took 0.1s, 3704352 effective words/s\n",
      "2023-11-21 14:04:10,374:INFO - EPOCH 30: training on 281697 raw words (239976 effective words) took 0.1s, 3486060 effective words/s\n",
      "2023-11-21 14:04:10,441:INFO - EPOCH 31: training on 281697 raw words (239952 effective words) took 0.1s, 3620123 effective words/s\n",
      "2023-11-21 14:04:10,510:INFO - EPOCH 32: training on 281697 raw words (239947 effective words) took 0.1s, 3505516 effective words/s\n",
      "2023-11-21 14:04:10,576:INFO - EPOCH 33: training on 281697 raw words (239936 effective words) took 0.1s, 3669097 effective words/s\n",
      "2023-11-21 14:04:10,646:INFO - EPOCH 34: training on 281697 raw words (239912 effective words) took 0.1s, 3506821 effective words/s\n",
      "2023-11-21 14:04:10,713:INFO - EPOCH 35: training on 281697 raw words (239959 effective words) took 0.1s, 3640106 effective words/s\n",
      "2023-11-21 14:04:10,783:INFO - EPOCH 36: training on 281697 raw words (239994 effective words) took 0.1s, 3464198 effective words/s\n",
      "2023-11-21 14:04:10,850:INFO - EPOCH 37: training on 281697 raw words (240059 effective words) took 0.1s, 3638875 effective words/s\n",
      "2023-11-21 14:04:10,919:INFO - EPOCH 38: training on 281697 raw words (239957 effective words) took 0.1s, 3531670 effective words/s\n",
      "2023-11-21 14:04:10,988:INFO - EPOCH 39: training on 281697 raw words (239706 effective words) took 0.1s, 3540031 effective words/s\n",
      "2023-11-21 14:04:11,057:INFO - EPOCH 40: training on 281697 raw words (239969 effective words) took 0.1s, 3530576 effective words/s\n",
      "2023-11-21 14:04:11,124:INFO - EPOCH 41: training on 281697 raw words (240068 effective words) took 0.1s, 3621764 effective words/s\n",
      "2023-11-21 14:04:11,192:INFO - EPOCH 42: training on 281697 raw words (239842 effective words) took 0.1s, 3533955 effective words/s\n",
      "2023-11-21 14:04:11,255:INFO - EPOCH 43: training on 281697 raw words (239870 effective words) took 0.1s, 3892674 effective words/s\n",
      "2023-11-21 14:04:11,317:INFO - EPOCH 44: training on 281697 raw words (239912 effective words) took 0.1s, 3940420 effective words/s\n",
      "2023-11-21 14:04:11,388:INFO - EPOCH 45: training on 281697 raw words (239907 effective words) took 0.1s, 3430565 effective words/s\n",
      "2023-11-21 14:04:11,455:INFO - EPOCH 46: training on 281697 raw words (239977 effective words) took 0.1s, 3667651 effective words/s\n",
      "2023-11-21 14:04:11,524:INFO - EPOCH 47: training on 281697 raw words (239911 effective words) took 0.1s, 3497604 effective words/s\n",
      "2023-11-21 14:04:11,591:INFO - EPOCH 48: training on 281697 raw words (239945 effective words) took 0.1s, 3633940 effective words/s\n",
      "2023-11-21 14:04:11,661:INFO - EPOCH 49: training on 281697 raw words (239981 effective words) took 0.1s, 3489679 effective words/s\n",
      "2023-11-21 14:04:11,727:INFO - EPOCH 50: training on 281697 raw words (239956 effective words) took 0.1s, 3665785 effective words/s\n",
      "2023-11-21 14:04:11,796:INFO - EPOCH 51: training on 281697 raw words (239970 effective words) took 0.1s, 3545750 effective words/s\n",
      "2023-11-21 14:04:11,862:INFO - EPOCH 52: training on 281697 raw words (239900 effective words) took 0.1s, 3666757 effective words/s\n",
      "2023-11-21 14:04:11,931:INFO - EPOCH 53: training on 281697 raw words (239855 effective words) took 0.1s, 3527539 effective words/s\n",
      "2023-11-21 14:04:11,998:INFO - EPOCH 54: training on 281697 raw words (239949 effective words) took 0.1s, 3672768 effective words/s\n",
      "2023-11-21 14:04:12,068:INFO - EPOCH 55: training on 281697 raw words (239759 effective words) took 0.1s, 3472101 effective words/s\n",
      "2023-11-21 14:04:12,134:INFO - EPOCH 56: training on 281697 raw words (239753 effective words) took 0.1s, 3676153 effective words/s\n",
      "2023-11-21 14:04:12,202:INFO - EPOCH 57: training on 281697 raw words (239985 effective words) took 0.1s, 3567907 effective words/s\n",
      "2023-11-21 14:04:12,271:INFO - EPOCH 58: training on 281697 raw words (239945 effective words) took 0.1s, 3580857 effective words/s\n",
      "2023-11-21 14:04:12,339:INFO - EPOCH 59: training on 281697 raw words (239990 effective words) took 0.1s, 3541284 effective words/s\n",
      "2023-11-21 14:04:12,407:INFO - EPOCH 60: training on 281697 raw words (239840 effective words) took 0.1s, 3613769 effective words/s\n",
      "2023-11-21 14:04:12,475:INFO - EPOCH 61: training on 281697 raw words (239962 effective words) took 0.1s, 3552979 effective words/s\n",
      "2023-11-21 14:04:12,542:INFO - EPOCH 62: training on 281697 raw words (239761 effective words) took 0.1s, 3654728 effective words/s\n",
      "2023-11-21 14:04:12,611:INFO - EPOCH 63: training on 281697 raw words (239954 effective words) took 0.1s, 3539708 effective words/s\n",
      "2023-11-21 14:04:12,678:INFO - EPOCH 64: training on 281697 raw words (239952 effective words) took 0.1s, 3637336 effective words/s\n",
      "2023-11-21 14:04:12,747:INFO - EPOCH 65: training on 281697 raw words (240015 effective words) took 0.1s, 3524400 effective words/s\n",
      "2023-11-21 14:04:12,813:INFO - EPOCH 66: training on 281697 raw words (239933 effective words) took 0.1s, 3635840 effective words/s\n",
      "2023-11-21 14:04:12,882:INFO - EPOCH 67: training on 281697 raw words (240011 effective words) took 0.1s, 3553661 effective words/s\n",
      "2023-11-21 14:04:12,948:INFO - EPOCH 68: training on 281697 raw words (239886 effective words) took 0.1s, 3679327 effective words/s\n",
      "2023-11-21 14:04:13,017:INFO - EPOCH 69: training on 281697 raw words (239840 effective words) took 0.1s, 3532954 effective words/s\n",
      "2023-11-21 14:04:13,084:INFO - EPOCH 70: training on 281697 raw words (240107 effective words) took 0.1s, 3640918 effective words/s\n",
      "2023-11-21 14:04:13,153:INFO - EPOCH 71: training on 281697 raw words (239812 effective words) took 0.1s, 3515478 effective words/s\n",
      "2023-11-21 14:04:13,220:INFO - EPOCH 72: training on 281697 raw words (239895 effective words) took 0.1s, 3644378 effective words/s\n",
      "2023-11-21 14:04:13,287:INFO - EPOCH 73: training on 281697 raw words (239743 effective words) took 0.1s, 3614359 effective words/s\n",
      "2023-11-21 14:04:13,349:INFO - EPOCH 74: training on 281697 raw words (239831 effective words) took 0.1s, 3886276 effective words/s\n",
      "2023-11-21 14:04:13,412:INFO - EPOCH 75: training on 281697 raw words (240011 effective words) took 0.1s, 3884608 effective words/s\n",
      "2023-11-21 14:04:13,475:INFO - EPOCH 76: training on 281697 raw words (239860 effective words) took 0.1s, 3870853 effective words/s\n",
      "2023-11-21 14:04:13,538:INFO - EPOCH 77: training on 281697 raw words (239871 effective words) took 0.1s, 3884247 effective words/s\n",
      "2023-11-21 14:04:13,600:INFO - EPOCH 78: training on 281697 raw words (239967 effective words) took 0.1s, 3925321 effective words/s\n",
      "2023-11-21 14:04:13,663:INFO - EPOCH 79: training on 281697 raw words (239819 effective words) took 0.1s, 3898094 effective words/s\n",
      "2023-11-21 14:04:13,725:INFO - EPOCH 80: training on 281697 raw words (240004 effective words) took 0.1s, 3902583 effective words/s\n",
      "2023-11-21 14:04:13,793:INFO - EPOCH 81: training on 281697 raw words (239802 effective words) took 0.1s, 3613335 effective words/s\n",
      "2023-11-21 14:04:13,859:INFO - EPOCH 82: training on 281697 raw words (239930 effective words) took 0.1s, 3679043 effective words/s\n",
      "2023-11-21 14:04:13,928:INFO - EPOCH 83: training on 281697 raw words (239939 effective words) took 0.1s, 3546215 effective words/s\n",
      "2023-11-21 14:04:13,997:INFO - EPOCH 84: training on 281697 raw words (239891 effective words) took 0.1s, 3519091 effective words/s\n",
      "2023-11-21 14:04:14,066:INFO - EPOCH 85: training on 281697 raw words (239746 effective words) took 0.1s, 3562001 effective words/s\n",
      "2023-11-21 14:04:14,133:INFO - EPOCH 86: training on 281697 raw words (239999 effective words) took 0.1s, 3624172 effective words/s\n",
      "2023-11-21 14:04:14,201:INFO - EPOCH 87: training on 281697 raw words (239928 effective words) took 0.1s, 3581518 effective words/s\n",
      "2023-11-21 14:04:14,267:INFO - EPOCH 88: training on 281697 raw words (239982 effective words) took 0.1s, 3691697 effective words/s\n",
      "2023-11-21 14:04:14,335:INFO - EPOCH 89: training on 281697 raw words (239928 effective words) took 0.1s, 3524164 effective words/s\n",
      "2023-11-21 14:04:14,401:INFO - EPOCH 90: training on 281697 raw words (239817 effective words) took 0.1s, 3680944 effective words/s\n",
      "2023-11-21 14:04:14,470:INFO - EPOCH 91: training on 281697 raw words (239976 effective words) took 0.1s, 3548637 effective words/s\n",
      "2023-11-21 14:04:14,536:INFO - EPOCH 92: training on 281697 raw words (239893 effective words) took 0.1s, 3666542 effective words/s\n",
      "2023-11-21 14:04:14,606:INFO - EPOCH 93: training on 281697 raw words (239947 effective words) took 0.1s, 3531114 effective words/s\n",
      "2023-11-21 14:04:14,672:INFO - EPOCH 94: training on 281697 raw words (239897 effective words) took 0.1s, 3687976 effective words/s\n",
      "2023-11-21 14:04:14,740:INFO - EPOCH 95: training on 281697 raw words (239930 effective words) took 0.1s, 3553095 effective words/s\n",
      "2023-11-21 14:04:14,808:INFO - EPOCH 96: training on 281697 raw words (240016 effective words) took 0.1s, 3644260 effective words/s\n",
      "2023-11-21 14:04:14,876:INFO - EPOCH 97: training on 281697 raw words (239800 effective words) took 0.1s, 3557395 effective words/s\n",
      "2023-11-21 14:04:14,942:INFO - EPOCH 98: training on 281697 raw words (239842 effective words) took 0.1s, 3680165 effective words/s\n",
      "2023-11-21 14:04:15,010:INFO - EPOCH 99: training on 281697 raw words (239839 effective words) took 0.1s, 3554324 effective words/s\n",
      "2023-11-21 14:04:15,011:INFO - Doc2Vec lifecycle event {'msg': 'training on 28169700 raw words (23991711 effective words) took 6.7s, 3556553 effective words/s', 'datetime': '2023-11-21T14:04:15.011096', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-11-21 14:04:15,011:INFO - Doc2Vec lifecycle event {'fname_or_handle': 'jesc102_model.d2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-11-21T14:04:15.011383', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'saving'}\n",
      "2023-11-21 14:04:15,011:INFO - not storing attribute cum_table\n",
      "2023-11-21 14:04:15,019:INFO - saved jesc102_model.d2v\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(vector_size=50, window=5, min_count=5, workers=4, epochs=100)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"jesc102_model.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77297d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 14:04:20,062:INFO - loading Doc2Vec object from jesc102_model.d2v\n",
      "2023-11-21 14:04:20,068:INFO - loading dv recursively from jesc102_model.d2v.dv.* with mmap=None\n",
      "2023-11-21 14:04:20,069:INFO - loading wv recursively from jesc102_model.d2v.wv.* with mmap=None\n",
      "2023-11-21 14:04:20,070:INFO - setting ignored attribute cum_table to None\n",
      "2023-11-21 14:04:20,118:INFO - Doc2Vec lifecycle event {'fname': 'jesc102_model.d2v', 'datetime': '2023-11-21T14:04:20.118141', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Doc2Vec.load(\"jesc102_model.d2v\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "273d86aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.17841513 -0.83037126  0.148273    1.4223593  -0.70772773  0.97898453\n",
      "  0.26453003  1.6398998  -1.7245947  -0.5404066  -1.0058852  -2.69086\n",
      "  1.1830423   0.09351102 -0.7664929  -0.02747363 -0.05803459 -0.71683\n",
      "  0.6335592  -2.0265644  -0.57418376  1.0851971   1.030711   -0.66408634\n",
      "  1.8216689   0.56689733  0.09521258 -0.7763474  -3.867591    1.059178\n",
      "  0.70101964 -0.22540908  0.07051926 -0.48160976  1.9386044  -0.24122363\n",
      "  0.8060256   0.80889916 -1.0189078  -0.10906455  2.1755433   0.5604259\n",
      "  0.20201609 -1.2408243   1.318136   -0.9532678  -1.6858147  -0.24810037\n",
      "  0.43620276  2.1151645 ]\n"
     ]
    }
   ],
   "source": [
    "# Infer a vector for a new sentence \n",
    "vector_sentence = model.infer_vector(preprocess_sentence(response_by_LLM))\n",
    "print(vector_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00da0ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2925ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "969e45f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04616312e-02, -1.19582536e-02, -1.97615083e-02, ...,\n",
       "        -4.11469443e-03,  1.08264564e-02, -1.60051417e-02],\n",
       "       [ 6.85736611e-02, -2.09120974e-01,  9.37654078e-02, ...,\n",
       "        -5.29735863e-01,  5.10803163e-01, -1.93054885e-01],\n",
       "       [ 3.96974981e-02, -6.17106199e-01, -2.03737959e-01, ...,\n",
       "        -3.06010187e-01,  4.10984129e-01,  1.84191912e-01],\n",
       "       ...,\n",
       "       [ 1.08481646e+00, -9.70334530e-01, -2.29545355e+00, ...,\n",
       "         2.55767870e+00,  1.08251180e-02,  2.37414169e+00],\n",
       "       [-1.03697062e+00, -2.17075372e+00,  5.36548972e-01, ...,\n",
       "         2.35963655e+00, -9.86251295e-01,  2.32671213e+00],\n",
       "       [ 1.43055069e+00, -5.37183619e+00, -6.47499800e-01, ...,\n",
       "         2.63408661e+00, -1.73421216e+00,  2.27383208e+00]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465620f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "186a158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e3a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3cbced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "vector_sentence = np.array(vector_sentence)\n",
    "vector_sentence_2d = vector_sentence.reshape(1, -1)\n",
    "\n",
    "similarities = cosine_similarity(vector_sentence_2d, vectors)\n",
    "\n",
    "# Find the index of the most similar paragraph\n",
    "most_similar_index = np.argmax(similarities)\n",
    "print(most_similar_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e44932e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 14:15:41,785:INFO - HTTP Request: GET https://tmwcifbhcnkaiqsldmlz.supabase.co/rest/v1/table_recent?select=text&source=eq.31 \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['12 Chapter 1. Introduction\\nSupercomputer\\n Personal Computer\\n Human Brain\\nComputational units\\n 104CPUs, 1012transistors\\n 4C P U s , 109transistors\\n 1011neurons\\nStorage units\\n 1014bits RAM\\n 1011bits RAM\\n 1011neurons\\n1015bits disk\\n 1013bits disk\\n 1014synapses\\nCycle time\\n 10−9sec\\n 10−9sec\\n 10−3sec\\nOperations/sec\\n 1015\\n1010\\n1017\\nMemory updates/sec\\n 1014\\n1010\\n1014\\nFigure 1.3 A crude comparison of the raw computational resources available to the IBM\\nBLUE GENE supercomputer, a typical personal computer of 2008, and the human brain. The\\nbrain’s numbers are essentially ﬁxed, whereas the supercomputer’s numbers have been in-creasing by a factor of 10 every 5 years or so, allowing it to achieve rough parity with thebrain. The personal computer lags behind on all metrics except cycle time.\\nThe only real alternative theory is mysticism: that minds operate in some mystical realm thatis beyond physical science.\\nBrains and digital computers have somewhat different properties. Figure 1.3 shows that\\ncomputers have a cycle time that is a million times faster than a brain. The brain makes upfor that with far more storage and interconnection than even a high-end personal computer,although the largest supercomputers have a capacity that is similar to the brain’s. (It shouldbe noted, however, that the brain does not seem to use all of its neurons simultaneously.)Futurists make much of these numbers, pointing to an approaching singularity at which\\nSINGULARITY\\ncomputers reach a superhuman level of performance (Vinge, 1993; Kurzweil, 2005), but theraw comparisons are not especially informative. Even with a computer of virtually unlimitedcapacity, we still would not know how to achieve the brain’s level of intelligence.\\n1.2.5 Psychology\\n•How do humans and animals think and act?\\nThe origins of scientiﬁc psychology are usually traced to the work of the German physi-cist Hermann von Helmholtz (1821–1894) and his student Wilhelm Wundt (1832–1920).Helmholtz applied the scientiﬁc method to the study of human vision, and his Handbook\\nof Physiological Optics is even now described as “the single most important treatise on the\\nphysics and physiology of human vision” (Nalwa, 1993, p.15). In 1879, Wundt opened theﬁrst laboratory of experimental psychology, at the University of Leipzig. Wundt insistedon carefully controlled experiments in which his workers would perform a perceptual or as-sociative task while introspecting on their thought processes. The careful controls went along way toward making psychology a science, but the subjective nature of the data madeit unlikely that an experimenter would ever disconﬁrm his or her own theories. Biologistsstudying animal behavior, on the other hand, lacked introspective data and developed an ob-jective methodology, as described by H. S. Jennings (1906) in his inﬂuential work Behavior of\\nthe Lower Organisms . Applying this viewpoint to humans, the behaviorism movement, led\\nBEHAVIORISM\\nby John Watson (1878–1958), rejected anytheory involving mental processes on the grounds']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from supabase import create_client, Client\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Supabase details\n",
    "url: str = \"https://tmwcifbhcnkaiqsldmlz.supabase.co\"\n",
    "key: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRtd2NpZmJoY25rYWlxc2xkbWx6Iiwicm9sZSI6ImFub24iLCJpYXQiOjE2OTkwNDI0MzAsImV4cCI6MjAxNDYxODQzMH0.tyy9IRYaXs7wkztSFAPUPSKGVz7Ng20xb_QCfWrObws\"\n",
    "\n",
    "# Create a client to connect to Supabase\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# Query to get vectors from your table (modify table name and column as per your schema)\n",
    "data = supabase.table(\"table_recent\").select(\"text\").eq(\"source\", most_similar_index).execute()\n",
    "\n",
    "# Assuming vectors are stored as arrays and not serialized\n",
    "relevant_text_from_source = [row['text'] for row in data.data]\n",
    "\n",
    "print(relevant_text_from_source)\n",
    "\n",
    "# Now 'vectors' contains the vectors from your table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f65b84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376f1c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b8ad62b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continue_conversation took 2.833407 seconds to execute.\n",
      "Final check: information not found in source\n"
     ]
    }
   ],
   "source": [
    "# Define a function to continue the conversation\n",
    "import time\n",
    "import re\n",
    "\n",
    "response = \"Wundt opened theﬁrst laboratory of experimental psychology, at the University of Leipzig. Wundt insistedon carefully controlled experiments in which his workers would perform a perceptual or as-sociative task while introspecting on their thought processes. The careful controls went along way toward making psychology a science\"\n",
    "def time_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.6f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@time_decorator\n",
    "def continue_conversation(prompt):\n",
    "    response = openai.ChatCompletion.create( model=\"gpt-4\", top_p=0,\n",
    "                                        messages=prompt,\n",
    "                                        temperature=0 )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "# Initial user prompt and system prompt\n",
    "\n",
    "\n",
    "initial_user_prompt=\" Define Machine Learning in short\"\n",
    "new_system_prompt = f\"Compare the following response with the provided source text and determine its accuracy. \" \\\n",
    "                     f\"Response: '{response_by_LLM}'. \" \\\n",
    "                     f\"Source Text: '{relevant_text_from_source}'. \" \\\n",
    "                     f\"If the response accurately reflects the source text, reply 'accurate'. \" \\\n",
    "                     f\"If it does not, reply 'inaccurate'. \" \\\n",
    "                     f\"If the response's content is not found in the source text, reply 'information not found in source'.\"\\\n",
    "                     f\"If the response is not relevant to question, reply 'response is irrelevant to question'. \"\n",
    "\n",
    "\n",
    "    \n",
    "history = [\n",
    "    {\"role\": \"system\", \"content\": new_system_prompt},\n",
    "    {\"role\": \"user\", \"content\": initial_user_prompt}]\n",
    "\n",
    "response = continue_conversation(history)\n",
    "print(\"Final check:\", response)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96b442a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

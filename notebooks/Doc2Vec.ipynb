{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a0cf0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import httpx\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from supabase import create_client\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from IPython.display import display\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76147f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page you want to scrape\n",
    "url = 'https://www.geeksforgeeks.org/machine-learning/'\n",
    "\n",
    "\n",
    "# Send GET request to the webpage\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7253dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Supabase client\n",
    "supabase_url = \"https://mufqacshyjgmzivznuwo.supabase.co\"\n",
    "supabase_key = \"Please add your key here\"\n",
    "client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "# change the table name as required\n",
    "table_name = \"combined_test\" \n",
    "\n",
    "headers = {\n",
    "    \"apikey\": supabase_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "endpoint = f\"{supabase_url}/rest/v1/combined_test\"\n",
    "\n",
    "# Please add your open ai key\n",
    "openai.api_key = \"Please add your key here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "680fe9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    # Remove non-alphabetic characters and stopwords\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Download required NLTK resources, if not already downloaded\n",
    "def download_nltk_resources():\n",
    "    nltk.download('punkt')      # For tokenization\n",
    "    nltk.download('stopwords')  # For removing stop words\n",
    "\n",
    "# Ensure NLTK resources are downloaded before proceeding\n",
    "download_nltk_resources()\n",
    "\n",
    "\n",
    "def extract_text_by_page(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            # Extract text from page and preprocess\n",
    "            text = page.extract_text()\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            # Preprocess each sentence and combine into one list of words for the page\n",
    "            words = [preprocess_sentence(sentence) for sentence in sentences]\n",
    "            words = [word for sublist in words for word in sublist]  # Flatten the list\n",
    "            yield TaggedDocument(words=words, tags=[str(i)])\n",
    "            \n",
    "pdf_path = 'C:/Users/devas/Downloads/AI-book.pdf'\n",
    "tagged_data = list(extract_text_by_page(pdf_path))\n",
    "\n",
    "### Divide the data in chunks of size 1000\n",
    "def chunk_section(section, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        length_function=len)\n",
    "    chunks = text_splitter.create_documents(\n",
    "        texts=[section[\"text\"]], \n",
    "        metadatas=[{\"source\": section[\"source\"]}])\n",
    "    return [{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a8da4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 14:10:11,422:INFO - Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d50,n5,w5,mc5,s0.001,t4)', 'datetime': '2023-12-01T14:10:11.422960', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'created'}\n",
      "2023-12-01 14:10:11,425:INFO - collecting all words and their counts\n",
      "2023-12-01 14:10:11,427:INFO - PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2023-12-01 14:10:11,571:INFO - collected 28892 word types and 1153 unique tags from a corpus of 1153 examples and 281697 words\n",
      "2023-12-01 14:10:11,572:INFO - Creating a fresh vocabulary\n",
      "2023-12-01 14:10:11,647:INFO - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 6246 unique words (21.618441090959436%% of original 28892, drops 22646)', 'datetime': '2023-12-01T14:10:11.647520', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-01 14:10:11,648:INFO - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 248011 word corpus (88.04176118311518%% of original 281697, drops 33686)', 'datetime': '2023-12-01T14:10:11.648548', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-01 14:10:11,754:INFO - deleting the raw counts dictionary of 28892 items\n",
      "2023-12-01 14:10:11,756:INFO - sample=0.001 downsamples 32 most-common words\n",
      "2023-12-01 14:10:11,759:INFO - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 238749.98766387967 word corpus (96.3%% of prior 248011)', 'datetime': '2023-12-01T14:10:11.759330', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'prepare_vocab'}\n",
      "2023-12-01 14:10:11,916:INFO - estimated required memory for 6246 words and 50 dimensions: 6082600 bytes\n",
      "2023-12-01 14:10:11,917:INFO - resetting layer weights\n",
      "2023-12-01 14:10:11,926:INFO - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6246 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-12-01T14:10:11.926864', 'gensim': '4.1.2', 'python': '3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.22621-SP0', 'event': 'train'}\n",
      "2023-12-01 14:10:12,349:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:12,355:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:12,359:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:12,368:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:12,370:INFO - EPOCH - 1 : training on 281697 raw words (239888 effective words) took 0.4s, 551743 effective words/s\n",
      "2023-12-01 14:10:12,735:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:12,754:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:12,757:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:12,761:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:12,762:INFO - EPOCH - 2 : training on 281697 raw words (239854 effective words) took 0.4s, 620458 effective words/s\n",
      "2023-12-01 14:10:13,198:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:13,199:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:13,205:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:13,219:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:13,220:INFO - EPOCH - 3 : training on 281697 raw words (239930 effective words) took 0.5s, 531000 effective words/s\n",
      "2023-12-01 14:10:13,590:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:13,596:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:13,598:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:13,608:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:13,610:INFO - EPOCH - 4 : training on 281697 raw words (239999 effective words) took 0.4s, 627030 effective words/s\n",
      "2023-12-01 14:10:14,010:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:14,017:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:14,028:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:14,038:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:14,039:INFO - EPOCH - 5 : training on 281697 raw words (239902 effective words) took 0.4s, 572916 effective words/s\n",
      "2023-12-01 14:10:14,474:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:14,479:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:14,481:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:14,490:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:14,492:INFO - EPOCH - 6 : training on 281697 raw words (239905 effective words) took 0.4s, 541620 effective words/s\n",
      "2023-12-01 14:10:14,909:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:14,920:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:14,929:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:14,932:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:14,933:INFO - EPOCH - 7 : training on 281697 raw words (240001 effective words) took 0.4s, 550326 effective words/s\n",
      "2023-12-01 14:10:15,295:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:15,303:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:15,305:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:15,307:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:15,308:INFO - EPOCH - 8 : training on 281697 raw words (239814 effective words) took 0.4s, 649015 effective words/s\n",
      "2023-12-01 14:10:15,661:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:15,665:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:15,669:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:15,675:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:15,676:INFO - EPOCH - 9 : training on 281697 raw words (240044 effective words) took 0.4s, 660489 effective words/s\n",
      "2023-12-01 14:10:16,132:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:16,141:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:16,143:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:16,146:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:16,147:INFO - EPOCH - 10 : training on 281697 raw words (239922 effective words) took 0.5s, 516401 effective words/s\n",
      "2023-12-01 14:10:16,534:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:16,537:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:16,539:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:16,551:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:16,552:INFO - EPOCH - 11 : training on 281697 raw words (239948 effective words) took 0.4s, 601027 effective words/s\n",
      "2023-12-01 14:10:16,931:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:16,936:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:16,938:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:16,948:INFO - worker thread finished; awaiting finish of 0 more threads\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-01 14:10:16,949:INFO - EPOCH - 12 : training on 281697 raw words (239982 effective words) took 0.4s, 611899 effective words/s\n",
      "2023-12-01 14:10:17,331:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:17,337:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:17,341:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:17,350:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:17,351:INFO - EPOCH - 13 : training on 281697 raw words (239910 effective words) took 0.4s, 603573 effective words/s\n",
      "2023-12-01 14:10:17,735:INFO - worker thread finished; awaiting finish of 3 more threads\n",
      "2023-12-01 14:10:17,739:INFO - worker thread finished; awaiting finish of 2 more threads\n",
      "2023-12-01 14:10:17,743:INFO - worker thread finished; awaiting finish of 1 more threads\n",
      "2023-12-01 14:10:17,751:INFO - worker thread finished; awaiting finish of 0 more threads\n",
      "2023-12-01 14:10:17,753:INFO - EPOCH - 14 : training on 281697 raw words (239889 effective words) took 0.4s, 606680 effective words/s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_49892\\2885974562.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDoc2Vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvector_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Save the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    512\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_doctags'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 514\u001b[1;33m         super(Doc2Vec, self).train(\n\u001b[0m\u001b[0;32m    515\u001b[0m             \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    516\u001b[0m             \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, corpus_iterable, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1067\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1068\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mcorpus_iterable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1069\u001b[1;33m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[0;32m   1070\u001b[0m                     \u001b[0mcorpus_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m                     \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mqueue_factor\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mqueue_factor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m   1428\u001b[0m             \u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1430\u001b[1;33m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[0;32m   1431\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1432\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreport_delay\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreport_delay\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_corpus_file_mode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1284\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1285\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1286\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1287\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    169\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    170\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 171\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    172\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    173\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    311\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 312\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(vector_size=50, window=5, min_count=5, workers=4, epochs=100)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"jesc102_model.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f06ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"jesc102_model.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c897b24a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections= []\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the <ul> tag with the class 'leftBarList'\n",
    "    left_bar_list = soup.find('ul', class_='leftBarList')\n",
    "    \n",
    "    # Find all <a> tags within the left bar list\n",
    "    links = left_bar_list.find_all('a') if left_bar_list else []\n",
    "\n",
    "    # Loop through all found <a> tags to get their href attribute (URLs)\n",
    "    for link in links:\n",
    "        # Ensure that only full URLs are printed\n",
    "        href = link.get('href')\n",
    "        full_url = href if href.startswith('http') else f\"https://www.geeksforgeeks.org{href}\"\n",
    "        \n",
    "        page_response = requests.get(full_url)\n",
    "        if page_response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "             # Find all <p> tags on the page and get their text content\n",
    "            paragraphs = page_soup.find_all('p')\n",
    "            paragraph_texts = [p.get_text(separator='\\n', strip=True) for p in paragraphs]\n",
    "            \n",
    "            # Combine all paragraph texts into a single string\n",
    "            full_text = '\\n\\n'.join(paragraph_texts)\n",
    "            section = {\"text\": full_text, \"source\": full_url}\n",
    "            sections.append({\"text\": full_text, \"source\": full_url})\n",
    "            chunks_ds = chunk_section(section,chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "            for chunk in chunks_ds:\n",
    "                vector = model.infer_vector(preprocess_sentence(chunk[\"text\"])).tolist()\n",
    "                vector_data = json.dumps(vector)\n",
    "            \n",
    "                data_to_insert = {\n",
    "                    \"source\": chunk[\"source\"],\n",
    "                    \"text\" : chunk[\"text\"],\n",
    "                    \"vector\": vector_data \n",
    "                }\n",
    "            \n",
    "            response = httpx.post(endpoint, headers=headers, json=data_to_insert)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page from {full_url}. Status code: {page_response.status_code}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3794d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.6f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@time_decorator\n",
    "def continue_conversation(prompt):\n",
    "    response = openai.ChatCompletion.create( model=\"gpt-4\", top_p=0,\n",
    "                                        messages=prompt,\n",
    "                                        temperature=0 )\n",
    "    return response['choices'][0]['message']['content'].strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c2078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supabase details\n",
    "url: str = \"https://mufqacshyjgmzivznuwo.supabase.co\"\n",
    "key: str = \"Please add your key here\"\n",
    "\n",
    "# Create a client to connect to Supabase\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ids = []\n",
    "sources = []\n",
    "vectors = []\n",
    "\n",
    "response = supabase.table(\"vector_final\").select(\"id, source, vector\").execute()\n",
    "if response.data:\n",
    "    for row in response.data:\n",
    "        # Extracting each column's value and appending to the respective lists\n",
    "        ids.append(row['id'])\n",
    "        sources.append(row['source'])\n",
    "        vectors.append(row['vector'])  # Assuming 'vector' is already in the desired format (e.g., list of numbers)\n",
    "\n",
    "# Now, ids, sources, and vectors have the data from their respective columns.\n",
    "vectors = [ast.literal_eval(vector) for vector in vectors]\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373ef05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "truth = pd.read_excel('llm_testing.xlsx')\n",
    "test = pd.read_excel('questions.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f0ad64",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers = []\n",
    "for q in test[\"Question\"]:\n",
    "    top5_data = []\n",
    "    initial_user_prompt= q\n",
    "    new_system_prompt =\"Answer the question asked by the user\" \n",
    "\n",
    "    print(q)\n",
    "    \n",
    "    history = [\n",
    "        {\"role\": \"system\", \"content\": new_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": initial_user_prompt}]\n",
    "\n",
    "    response_by_LLM = continue_conversation(history)\n",
    "    vector_sentence = model.infer_vector(preprocess_sentence(response_by_LLM))\n",
    "    sr = []\n",
    "    \n",
    "    vector_sentence = np.array(vector_sentence)\n",
    "    vector_sentence_2d = vector_sentence.reshape(1, -1)\n",
    "\n",
    "    similarities = cosine_similarity(vector_sentence_2d, vectors)\n",
    "\n",
    "    # Find the index of the most similar paragraph\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    top_5_indexes = np.argsort(similarities[0])[-5:][::-1]\n",
    "    for index in top_5_indexes:\n",
    "        sr.append(sources[index])\n",
    "\n",
    "    res = []\n",
    "    for i in range(len(sr)):\n",
    "        data = supabase.table(\"vector_final\").select(\"text\").eq(\"source\", sr[i]).execute()\n",
    "        top5_data.append(data)\n",
    "\n",
    "        # Assuming vectors are stored as arrays and not serialized\n",
    "        relevant_text_from_source = [row['text'] for row in top5_data[i].data]\n",
    "    \n",
    "        initial_user_prompt= q\n",
    "        \n",
    "        new_system_prompt = f\"Compare the following response with the provided source text using semantic check and determine its accuracy. \" \\\n",
    "                     f\"Response: '{response_by_LLM}'. \" \\\n",
    "                     f\"Source Text: '{relevant_text_from_source}'. \" \\\n",
    "                     f\"If the response accurately reflects the source text, reply 'accurate'. \" \\\n",
    "                     f\"If the response does not accurately reflect the source text , reply 'inaccurate'. \" \\\n",
    "                     f\"If the response's content is not found in the source text, reply 'information not found in source'.\"\\\n",
    "                     f\"If the response is not relevant to question, reply 'response is irrelevant to question'. \"\n",
    "\n",
    "\n",
    "    \n",
    "        history = [\n",
    "            {\"role\": \"system\", \"content\": new_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": initial_user_prompt}]\n",
    "\n",
    "        response = continue_conversation(history)\n",
    "        res.append(response)\n",
    "        print(res)\n",
    "    if 'Accurate' in res:\n",
    "        answers.append(\"Final check: Accurate\")\n",
    "    else:\n",
    "        answers.append(\"Final check: \"+ response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810cbaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 1\n",
    "for i in range(len(answers)):\n",
    "    print(truth['Answer'][i])\n",
    "    print(answers[i])\n",
    "    if (truth['Answer'][i] == answers[i]):\n",
    "        acc += 1\n",
    "print(\"Accuracy : \", acc/ len(answers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5ea99e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not import azure.core python package.\n"
     ]
    }
   ],
   "source": [
    "#Make necessary imports\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import httpx\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from supabase import create_client\n",
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "from IPython.display import display\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b034f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the page you want to scrape\n",
    "url = 'https://www.geeksforgeeks.org/machine-learning/'\n",
    "\n",
    "\n",
    "# Send GET request to the webpage\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e3474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize the Supabase client\n",
    "supabase_url = \"https://mufqacshyjgmzivznuwo.supabase.co\"\n",
    "supabase_key = \"Please add your key here\"\n",
    "client = create_client(supabase_url, supabase_key)\n",
    "\n",
    "# change the table name as required\n",
    "table_name = \"combined_test\" \n",
    "\n",
    "headers = {\n",
    "    \"apikey\": supabase_key,\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "endpoint = f\"{supabase_url}/rest/v1/combined_test\"\n",
    "\n",
    "# Please add your open ai key\n",
    "openai.api_key = \"Please add your key here\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert data to embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebd0f114",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\devas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# This method preprocesses a given sentence by tokenizing it, converting it to lowercase,\n",
    "# and removing any tokens that are not alphabetic or are considered stopwords.\n",
    "def preprocess_sentence(sentence):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    # Remove non-alphabetic characters and stopwords\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Download required NLTK resources, if not already downloaded\n",
    "def download_nltk_resources():\n",
    "    nltk.download('punkt')      # For tokenization\n",
    "    nltk.download('stopwords')  # For removing stop words\n",
    "\n",
    "# Ensure NLTK resources are downloaded before proceeding\n",
    "download_nltk_resources()\n",
    "\n",
    "# This generator function opens a PDF and yields preprocessed text from each page as a TaggedDocument.\n",
    "# The TaggedDocument is suitable for training Doc2Vec models in Gensim.\n",
    "def extract_text_by_page(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            # Extract text from page and preprocess\n",
    "            text = page.extract_text()\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            # Preprocess each sentence and combine into one list of words for the page\n",
    "            words = [preprocess_sentence(sentence) for sentence in sentences]\n",
    "            words = [word for sublist in words for word in sublist]  # Flatten the list\n",
    "            # Yielding a TaggedDocument for each page\n",
    "            yield TaggedDocument(words=words, tags=[str(i)])\n",
    "            \n",
    "pdf_path = 'C:/Users/devas/Downloads/AI-book.pdf'\n",
    "tagged_data = list(extract_text_by_page(pdf_path))\n",
    "\n",
    "### Divide the data in chunks of size 1000\n",
    "def chunk_section(section, chunk_size, chunk_overlap):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"],   # Define separators for splitting text.\n",
    "        chunk_size=chunk_size,                # Maximum size of each chunk.\n",
    "        chunk_overlap=chunk_overlap,          # Number of characters to overlap between chunks.\n",
    "        length_function=len)\n",
    "\n",
    "    # Use the text splitter to divide the input text into chunks.\n",
    "    chunks = text_splitter.create_documents(\n",
    "        texts=[section[\"text\"]], \n",
    "        metadatas=[{\"source\": section[\"source\"]}])\n",
    "    # Return a list of dictionaries, each containing a chunk of text and its associated metadata.\n",
    "    return [{\"text\": chunk.page_content, \"source\": chunk.metadata[\"source\"]} for chunk in chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be819cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Doc2Vec model with specific parameters.\n",
    "# - vector_size: Dimensionality of the feature vectors.\n",
    "# - window: The maximum distance between the current and predicted word within a sentence.\n",
    "# - min_count: Ignores all words with total frequency lower than this.\n",
    "# - workers: Number of worker threads to train the model (faster training with multicore machines).\n",
    "# - epochs: Number of iterations (passes) over the corpus.\n",
    "model = Doc2Vec(vector_size=50, window=5, min_count=5, workers=4, epochs=100)\n",
    "\n",
    "# Build a vocabulary from the tagged data.\n",
    "model.build_vocab(tagged_data)\n",
    "\n",
    "# Train the Doc2Vec model on the tagged data.\n",
    "# - total_examples: Total number of documents.\n",
    "# - epochs: Number of iterations over the corpus\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"jesc102_model.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbe5f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec.load(\"jesc102_model.d2v\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data to supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f488bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sections= []\n",
    "if response.status_code == 200:\n",
    "    # Parse the HTML content of the page\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the <ul> tag with the class 'leftBarList'\n",
    "    left_bar_list = soup.find('ul', class_='leftBarList')\n",
    "    \n",
    "    # Find all <a> tags within the left bar list\n",
    "    links = left_bar_list.find_all('a') if left_bar_list else []\n",
    "\n",
    "    # Loop through all found <a> tags to get their href attribute (URLs)\n",
    "    for link in links:\n",
    "        # Ensure that only full URLs are printed\n",
    "        href = link.get('href')\n",
    "        full_url = href if href.startswith('http') else f\"https://www.geeksforgeeks.org{href}\"\n",
    "        \n",
    "        page_response = requests.get(full_url)\n",
    "        if page_response.status_code == 200:\n",
    "            # Parse the HTML content of the page\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "             # Find all <p> tags on the page and get their text content\n",
    "            paragraphs = page_soup.find_all('p')\n",
    "            paragraph_texts = [p.get_text(separator='\\n', strip=True) for p in paragraphs]\n",
    "            \n",
    "            # Combine all paragraph texts into a single string\n",
    "            full_text = '\\n\\n'.join(paragraph_texts)\n",
    "\n",
    "            # create a section with text and source field\n",
    "            section = {\"text\": full_text, \"source\": full_url}\n",
    "\n",
    "            # combine all sections with their respective sources\n",
    "            sections.append({\"text\": full_text, \"source\": full_url})\n",
    "            chunks_ds = chunk_section(section,chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "            # Load data chunks for each section to Supabase DB\n",
    "            for chunk in chunks_ds:\n",
    "                vector = model.infer_vector(preprocess_sentence(chunk[\"text\"])).tolist()\n",
    "                vector_data = json.dumps(vector)\n",
    "            \n",
    "                #Prepare the data to be inserted \n",
    "                data_to_insert = {\n",
    "                    \"source\": chunk[\"source\"],\n",
    "                    \"text\" : chunk[\"text\"],\n",
    "                    \"vector\": vector_data \n",
    "                }\n",
    "                \n",
    "            # Send a POST request to store the vector data\n",
    "            response = httpx.post(endpoint, headers=headers, json=data_to_insert)\n",
    "            \n",
    "        else:\n",
    "            print(f\"Failed to retrieve the page from {full_url}. Status code: {page_response.status_code}\")\n",
    "else:\n",
    "    print(f\"Failed to retrieve the webpage. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741ab14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A decorator function to measure and print the execution time of functions.\n",
    "def time_decorator(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        result = func(*args, **kwargs)\n",
    "        end_time = time.time()\n",
    "        elapsed_time = end_time - start_time\n",
    "        print(f\"{func.__name__} took {elapsed_time:.6f} seconds to execute.\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "# Function to continue a conversation using OpenAI's GPT-4.\n",
    "@time_decorator\n",
    "def continue_conversation(prompt):\n",
    "    response = openai.ChatCompletion.create( model=\"gpt-4\", top_p=0,\n",
    "                                        messages=prompt,\n",
    "                                        temperature=0 )\n",
    "    return response['choices'][0]['message']['content'].strip()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load knowledge source vectors from dupabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174eff84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supabase details\n",
    "url: str = \"https://mufqacshyjgmzivznuwo.supabase.co\"\n",
    "key: str = \"Please add your key here\"\n",
    "\n",
    "# Create a client to connect to Supabase\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "import ast\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ids = []\n",
    "sources = []\n",
    "vectors = []\n",
    "\n",
    "response = supabase.table(\"vector_final\").select(\"id, source, vector\").execute()\n",
    "if response.data:\n",
    "    for row in response.data:\n",
    "        # Extracting each column's value and appending to the respective lists\n",
    "        ids.append(row['id'])\n",
    "        sources.append(row['source'])\n",
    "        vectors.append(row['vector'])  \n",
    "\n",
    "# # Prepare the vectors for cosine similarity\n",
    "vectors = [ast.literal_eval(vector) for vector in vectors]\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30dce795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load ground truth and test data from Excel files\n",
    "truth = pd.read_excel('llm_testing.xlsx')\n",
    "test = pd.read_excel('questions.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253e00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main process for checking answers against a test dataset\n",
    "answers = []\n",
    "for q in test[\"Question\"]:\n",
    "    top5_data = []\n",
    "    initial_user_prompt= q\n",
    "    new_system_prompt =\"Answer the question asked by the user\" \n",
    "\n",
    "    print(q)\n",
    "    \n",
    "    history = [\n",
    "        {\"role\": \"system\", \"content\": new_system_prompt},\n",
    "        {\"role\": \"user\", \"content\": initial_user_prompt}]\n",
    "\n",
    "    # Get the answer of question from LLM\n",
    "    response_by_LLM = continue_conversation(history)\n",
    "    vector_sentence = model.infer_vector(preprocess_sentence(response_by_LLM))\n",
    "    sr = []\n",
    "    \n",
    "    # Convert the response to vector embeddings\n",
    "    vector_sentence = np.array(vector_sentence)\n",
    "    vector_sentence_2d = vector_sentence.reshape(1, -1)\n",
    "\n",
    "    # calculate the cosine similarity\n",
    "    similarities = cosine_similarity(vector_sentence_2d, vectors)\n",
    "\n",
    "    # Find the index of the most similar paragraph\n",
    "    most_similar_index = np.argmax(similarities)\n",
    "\n",
    "    top_5_indexes = np.argsort(similarities[0])[-5:][::-1]\n",
    "    for index in top_5_indexes:\n",
    "        sr.append(sources[index])\n",
    "\n",
    "    res = []\n",
    "    for i in range(len(sr)):\n",
    "        # fetch data for top 5 contexts from supabase\n",
    "        data = supabase.table(\"vector_final\").select(\"text\").eq(\"source\", sr[i]).execute()\n",
    "        top5_data.append(data)\n",
    "\n",
    "        # Extract the relevant text\n",
    "        relevant_text_from_source = [row['text'] for row in top5_data[i].data]\n",
    "    \n",
    "        initial_user_prompt= q\n",
    "        \n",
    "        #Prompt given to GPT4\n",
    "        new_system_prompt = f\"Compare the following response with the provided source text using semantic check and determine its accuracy. \" \\\n",
    "                     f\"Response: '{response_by_LLM}'. \" \\\n",
    "                     f\"Source Text: '{relevant_text_from_source}'. \" \\\n",
    "                     f\"If the response accurately reflects the source text, reply 'accurate'. \" \\\n",
    "                     f\"If the response does not accurately reflect the source text , reply 'inaccurate'. \" \\\n",
    "                     f\"If the response's content is not found in the source text, reply 'information not found in source'.\"\\\n",
    "                     f\"If the response is not relevant to question, reply 'response is irrelevant to question'. \"\n",
    "\n",
    "\n",
    "    \n",
    "        history = [\n",
    "            {\"role\": \"system\", \"content\": new_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": initial_user_prompt}]\n",
    "\n",
    "        response = continue_conversation(history)\n",
    "        res.append(response)\n",
    "        print(res)\n",
    "    \n",
    "    # If accurate answer is found in any of top 5 contexts, tag the answer as accurate\n",
    "    if 'Accurate' in res:\n",
    "        answers.append(\"Final check: Accurate\")\n",
    "    else:\n",
    "        answers.append(\"Final check: \"+ response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42e68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Claculate the accuracy\n",
    "# Compare answers from LLM with expected answers and calculate the accuracy\n",
    "acc = 1\n",
    "for i in range(len(answers)):\n",
    "    print(truth['Answer'][i])\n",
    "    print(answers[i])\n",
    "    if (truth['Answer'][i] == answers[i]):\n",
    "        acc += 1\n",
    "print(\"Accuracy : \", acc/ len(answers))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

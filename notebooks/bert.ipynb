{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dae50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers\n",
    "!pip install openai==0.28\n",
    "!pip install pdfminer.six\n",
    "!pip install httpx\n",
    "!pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09072dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make necessary imports\n",
    "import PyPDF2\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import httpx\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from supabase import create_client, Client\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "import openai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a8a625",
   "metadata": {},
   "source": [
    "## Configurable parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3870152c",
   "metadata": {},
   "outputs": [],
   "source": [
    "SUPABASE_URL = \"https://tmwcifbhcnkaiqsldmlz.supabase.co\"\n",
    "SUPABASE_SERVICE_API_KEY = \"please use your key here\"\n",
    "# ** NOTE ** : Also update supabase creds in \"query_supabase_with_embedding\" function\n",
    "\n",
    "openai_api_key = 'please use your key here'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90817e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = 'books/'\n",
    "supabase_table_name = \"books_pretrained_embeddings\"\n",
    "match_function_name = \"match_books_pretrained_embeddings\"  ## This is cosine similarity match function in supabase\n",
    "model_name = 'sentence-transformers/all-mpnet-base-v2'  ## Pretrained model name from Hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b67363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained Sentence Transformers model\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer(model_name)\n",
    "len(model.encode(\"hello\").tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e5c790",
   "metadata": {},
   "source": [
    "## Create table and match function in supabase by running following SQL code (Note: Use / update names for table and match function as you provide above )\n",
    "## Note: update value for vector beased on above vector length value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d275217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "-- Create table\n",
    "CREATE TABLE books_pretrained_embeddings (\n",
    "  source TEXT,\n",
    "  vector_data vector(768),  -- Array of 768 floats for vector data\n",
    "  text TEXT,\n",
    "  text_book TEXT,\n",
    "  page_number TEXT\n",
    ");\n",
    "\n",
    "-- Match function\n",
    "CREATE OR REPLACE FUNCTION match_books_pretrained_embeddings(\n",
    "  query_vector VECTOR,\n",
    "  threshold FLOAT,\n",
    "  match_count INT\n",
    ")\n",
    "RETURNS TABLE (\n",
    "  source TEXT,\n",
    "  text TEXT,\n",
    "  text_book TEXT,\n",
    "  page_number TEXT,\n",
    "  cosine_similarity_score FLOAT\n",
    ")\n",
    "LANGUAGE sql STABLE\n",
    "AS $$\n",
    "  SELECT\n",
    "    books_pretrained_embeddings.source,\n",
    "    books_pretrained_embeddings.text,\n",
    "    books_pretrained_embeddings.text_book,\n",
    "    books_pretrained_embeddings.page_number,\n",
    "    1 - (books_pretrained_embeddings.vector_data <=> query_vector) AS cosine_similarity_score\n",
    "  FROM books_pretrained_embeddings\n",
    "  WHERE 1 - (books_pretrained_embeddings.vector_data <=> query_vector) > threshold\n",
    "  ORDER BY cosine_similarity_score DESC\n",
    "  LIMIT match_count;\n",
    "$$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6972aa90",
   "metadata": {},
   "source": [
    "## Prep for knowledge / source text / groundtruth embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863076b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained Sentence Transformers model\n",
    "model = SentenceTransformer(model_name)\n",
    "\n",
    "# function to list files in a given directory\n",
    "def list_pdf_files(pdf_path):\n",
    "    \"\"\"Lists all PDF files in the given directory.\"\"\"\n",
    "    pdf_files = [file for file in os.listdir(pdf_path) if file.endswith('.pdf')]\n",
    "    return pdf_files\n",
    "\n",
    "# Tokenize and prepocess the sentence\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"Tokenizes and preprocesses a sentence.\"\"\"\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    return tokens\n",
    "\n",
    "\"\"\"Chunks text into sizes of about provided characters, ending at sentence boundaries.\n",
    "    Parameters:\n",
    "    text (str): The text to be chunked.\n",
    "    chunk_size (int): Target size for each chunk in characters.\n",
    "    overlap (int): Number of characters from the end of one chunk to overlap with the beginning of the next chunk.\n",
    "\n",
    "    Returns:\n",
    "    list: A list of text chunks.\n",
    "    \"\"\"\n",
    "def chunk_text(text, chunk_size=1200, overlap=100):\n",
    "    \"\"\"Chunks text into sizes of about provided characters, ending at sentence boundaries.\"\"\"\n",
    "    # Initialize an empty list to store the chunks\n",
    "    chunks = []\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    current_chunk = \"\"\n",
    "\n",
    "    # Iterate through each sentence in the tokenized text.\n",
    "    for sentence in sentences:\n",
    "        # If adding a sentence to the current chunk exceeds the chunk size\n",
    "        # and the current chunk is not empty, add the current chunk to the chunks list.\n",
    "        # Then, start a new chunk with the current sentence.\n",
    "        if len(current_chunk) + len(sentence) > chunk_size and len(current_chunk) > 0:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = sentence\n",
    "        else:\n",
    "            # If the current chunk plus the new sentence is within the limit,\n",
    "            # add the sentence to the current chunk\n",
    "            current_chunk += (\" \" if current_chunk else \"\") + sentence\n",
    "\n",
    "        # If the length of the current chunk exceeds the chunk size and there are already chunks in the list,\n",
    "        # take the last part of the previous chunk (as defined by the overlap) and add it to the current chunk.\n",
    "        # This creates an overlap between the end of the previous chunk and the start of the current chunk.\n",
    "        if len(current_chunk) > chunk_size and len(chunks) > 0:\n",
    "            last_chunk = chunks[-1]\n",
    "            overlap_text = last_chunk[-overlap:]\n",
    "            current_chunk = overlap_text + current_chunk\n",
    "\n",
    "    # After processing all sentences, if there is any remaining text in the current chunk, add it to the chunks list.\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Extracts text from each page of a PDF file and generates embeddings for the extracted text.\n",
    "\n",
    "    Parameters:\n",
    "    pdf_path (str): The path to the PDF file relative to the directory path.\n",
    "    dir_path (str): The directory path where the PDF file is located.\n",
    "\n",
    "    Yields:\n",
    "    tuple: A tuple containing the text chunk, its embedding, and the page number.\n",
    "    \"\"\"\n",
    "def extract_text_by_page(pdf_path, dir_path):\n",
    "    \"\"\"Extracts text from PDF pages using PDFMiner and generates embeddings.\"\"\"\n",
    "    file_path = dir_path + pdf_path\n",
    "    # Iterate through each page in the PDF file.\n",
    "    print(file_path)\n",
    "    for page_number, page_layout in enumerate(extract_pages(file_path)):\n",
    "        text = \"\"\n",
    "\n",
    "        # Iterate through each element in the page layout\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                # Append the text of the element to the overall text of the page.\n",
    "                text += element.get_text()\n",
    "\n",
    "        # Chunk the extracted text into smaller pieces.\n",
    "        text_chunks = chunk_text(text) \n",
    "\n",
    "        # Iterate through each chunk and generate its embedding.\n",
    "        for chunk in text_chunks:\n",
    "            vector = model.encode(chunk).tolist()  # Generate the embedding for the chunk of text and convert it to a list.\n",
    "\n",
    "            # Yield a tuple containing the text chunk, its embedding, and the page number.\n",
    "            yield chunk, vector, page_number\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc16c64",
   "metadata": {},
   "source": [
    "## To add data to supabase tables. Just run this once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839ab7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supabase setup\n",
    "headers = {\"apikey\": SUPABASE_SERVICE_API_KEY, \"Content-Type\": \"application/json\"}\n",
    "\n",
    "pdf_files = list_pdf_files(dir_path)\n",
    "\n",
    "# Process each PDF and send data to Supabase\n",
    "for pdf_path in pdf_files:\n",
    "    for chunk, vector, page_number in extract_text_by_page(pdf_path, dir_path):\n",
    "        \n",
    "        # Prepare the data to be inserted into the database.\n",
    "        data_to_insert = {\n",
    "            \"page_number\": str(page_number),\n",
    "            \"vector_data\": vector,\n",
    "            \"text\": chunk,\n",
    "            \"text_book\": pdf_path\n",
    "        }\n",
    "\n",
    "        # Construct the endpoint URL for posting data to Supabase.\n",
    "        endpoint = f\"{SUPABASE_URL}/rest/v1/{supabase_table_name}\"\n",
    "\n",
    "        # Use the httpx library to send a POST request to the Supabase endpoint.\n",
    "        response = httpx.post(endpoint, headers=headers, json=data_to_insert)\n",
    "        print(f\"Stored data for page {page_number} in '{pdf_path}': {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13581c",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a6b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\"\"\"\n",
    "    Queries the Supabase database using an embedding generated from the query document.\n",
    "\"\"\"\n",
    "def query_supabase_with_embedding(query_document, model):\n",
    "    # Generate a vector for the query document\n",
    "    query_vector = model.encode(query_document).tolist()  # Convert the text to an embedding list\n",
    "\n",
    "    # Supabase setup\n",
    "    SUPABASE_URL = \"https://tmwcifbhcnkaiqsldmlz.supabase.co\"\n",
    "    SUPABASE_SERVICE_API_KEY = \"Please add your key here\"\n",
    "    headers = {\n",
    "        \"apikey\": SUPABASE_SERVICE_API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    # Prepare the data payload\n",
    "    data = {\n",
    "        \"query_vector\": query_vector,\n",
    "        \"threshold\": 0.25,\n",
    "        \"match_count\": 5\n",
    "    }\n",
    "\n",
    "    # RPC endpoint for executing the function\n",
    "    endpoint = f\"{SUPABASE_URL}/rest/v1/rpc/{match_function_name}\"\n",
    "\n",
    "    # Execute the function via POST request\n",
    "    response = httpx.post(endpoint, headers=headers, json=data)\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        return [str(r) for r in results]\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}, {response.text}\")\n",
    "        return [\"error in RAG\"]\n",
    "\n",
    "# Initialize the SentenceTransformer model\n",
    "model = SentenceTransformer(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4924a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "\"\"\"\n",
    "    Extracts JSON or SQL code snippets from the provided text.\n",
    "\"\"\"\n",
    "def fetch_json_subparts(text):\n",
    "        # This regex captures content between the outermost curly braces\n",
    "        match = re.search(r'({.*})', text, re.DOTALL)\n",
    "        if match:\n",
    "            try:\n",
    "                json_obj = json.loads(match.group(1))\n",
    "                return json_obj\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"ERROR in JSON ! {e}\")\n",
    "                return \"error\"\n",
    "        # If not JSON, try to get SQL code\n",
    "        markdown_match = re.search(r'```json(.*?)```', text, re.DOTALL)\n",
    "        try:\n",
    "            json_obj = json.loads(markdown_match.group(1))\n",
    "            return json_obj\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"ERROR in JSON ! {e}\")\n",
    "            return \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a4135b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Continues a conversation with GPT-4 using the provided prompt.\n",
    "\"\"\"\n",
    "def continue_conversation(prompt):\n",
    "    response = openai.ChatCompletion.create( model=\"gpt-4-0314\", top_p=0,\n",
    "                                        messages=prompt,\n",
    "                                        temperature=0 )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    Check the factual accuracy of a query using OpenAI's GPT-4 with context from retrieved documents.\n",
    "\n",
    "    :param query: The query or statement to be checked.\n",
    "    :param documents: A list of documents that provide context for the query.\n",
    "    :param openai_api_key: Your OpenAI API key.\n",
    "    :return: The model's response regarding the factual accuracy of the query.\n",
    "    \"\"\"\n",
    "def check_fact_with_context(query, documents, openai_api_key):\n",
    "    \n",
    "    openai.api_key = openai_api_key\n",
    "\n",
    "    # Prepare the context by concatenating document contents\n",
    "    context = ' /n '.join(documents)  # You might need to adjust this based on how your documents are structured\n",
    "\n",
    "    # Formulate the prompt for GPT-4\n",
    "    prompt = f\"Based on the following information: {context}\\nCan you tell me if this statement is true or false? '{query}'\"\n",
    "    prompt = f\"\"\" # Your Job is to work as Fact checkers and carefuly understand given Source Text as ground truth information and query as claim or text needs to validate.\n",
    "                ## You should not use information outside of given Source Text and so step by step analysis in your mind and **strictly** provide output in following json format:\n",
    "                     ## Carefully evaluate info even for minute details\n",
    "                     ## Rule: You are not allowed to used accurate or inaccurate in classification if you dont knopw the reference where source fact lies\n",
    "                     f\"##Query / info to validate : '{query}'. \" \\\n",
    "                     f\"## Source Text: '{context}'. \" \\\n",
    "\n",
    "                    # Expected JSON output format:\n",
    "                     ```json\n",
    "                        {{\n",
    "                        classfication : \"label\" -  3 options for lables \"accurate\", \"inaccurate\" and \"information not found in source\" This can be classified only after reason\n",
    "                        reason : \"any supported reasoning based Source Text else say N/A\"\n",
    "                        reference: \" each document from Source text will have text_book and page_number info so provide that in string. if its label 3 info not found then provide N/A \"\n",
    "\n",
    "                        }}\n",
    "                     ```\n",
    "\n",
    "                     \"\"\"\n",
    "\n",
    "    history = [\n",
    "            {\"role\": \"system\", \"content\": \"You are Fact check evaluator!\" },\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    response = continue_conversation(history)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8699f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## ## Test the fact-checking process with examples\n",
    "test_examples = [\n",
    "    [\"Deductive reasoning is a form of illogical thinking that uses unrelated observations to arrive at a specific conclusion. This type of reasoning is common in descriptive science.\", \"inaccurate\"],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1054a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(test_examples):\n",
    "    query = example[0]\n",
    "    documents = query_supabase_with_embedding(query, model)\n",
    "\n",
    "    result = check_fact_with_context(query, documents, openai_api_key)\n",
    "    print(\"* \"*40)\n",
    "    print(\"example No :\", i+1)\n",
    "    print(query)\n",
    "    print(fetch_json_subparts(result))\n",
    "    print(\"expected output : \" , example[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

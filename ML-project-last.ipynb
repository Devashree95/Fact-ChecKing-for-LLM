{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc25164e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting supabase\n",
      "  Downloading supabase-2.0.3-py3-none-any.whl (11 kB)\n",
      "Collecting realtime<2.0.0,>=1.0.0\n",
      "  Downloading realtime-1.0.0-py3-none-any.whl (8.0 kB)\n",
      "Collecting supafunc<0.4.0,>=0.3.1\n",
      "  Downloading supafunc-0.3.1-py3-none-any.whl (6.1 kB)\n",
      "Collecting postgrest<0.14.0,>=0.10.8\n",
      "  Downloading postgrest-0.13.0-py3-none-any.whl (19 kB)\n",
      "Collecting storage3<0.7.0,>=0.5.3\n",
      "  Downloading storage3-0.6.1-py3-none-any.whl (15 kB)\n",
      "Collecting gotrue<2.0.0,>=1.3.0\n",
      "  Downloading gotrue-1.3.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 2.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting httpx<0.25.0,>=0.24.0\n",
      "  Downloading httpx-0.24.1-py3-none-any.whl (75 kB)\n",
      "\u001b[K     |████████████████████████████████| 75 kB 7.3 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.10 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from gotrue<2.0.0,>=1.3.0->supabase) (2.0.2)\n",
      "Collecting httpcore<0.18.0,>=0.15.0\n",
      "  Downloading httpcore-0.17.3-py3-none-any.whl (74 kB)\n",
      "\u001b[K     |████████████████████████████████| 74 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx<0.25.0,>=0.24.0->supabase) (2.10)\n",
      "Requirement already satisfied: certifi in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx<0.25.0,>=0.24.0->supabase) (2022.6.15)\n",
      "Requirement already satisfied: sniffio in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx<0.25.0,>=0.24.0->supabase) (1.3.0)\n",
      "Collecting anyio<5.0,>=3.0\n",
      "  Downloading anyio-4.0.0-py3-none-any.whl (83 kB)\n",
      "\u001b[K     |████████████████████████████████| 83 kB 8.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: h11<0.15,>=0.13 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpcore<0.18.0,>=0.15.0->httpx<0.25.0,>=0.24.0->supabase) (0.14.0)\n",
      "Collecting exceptiongroup>=1.0.2\n",
      "  Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: deprecation<3.0.0,>=2.1.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from postgrest<0.14.0,>=0.10.8->supabase) (2.1.0)\n",
      "Collecting strenum<0.5.0,>=0.4.9\n",
      "  Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
      "Requirement already satisfied: packaging in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from deprecation<3.0.0,>=2.1.0->postgrest<0.14.0,>=0.10.8->supabase) (23.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue<2.0.0,>=1.3.0->supabase) (4.7.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue<2.0.0,>=1.3.0->supabase) (0.5.0)\n",
      "Requirement already satisfied: pydantic-core==2.1.2 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from pydantic<3,>=1.10->gotrue<2.0.0,>=1.3.0->supabase) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.1 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from realtime<2.0.0,>=1.0.0->supabase) (2.8.2)\n",
      "Collecting websockets<11.0,>=10.3\n",
      "  Using cached websockets-10.4-cp310-cp310-macosx_11_0_arm64.whl (97 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.8.1->realtime<2.0.0,>=1.0.0->supabase) (1.16.0)\n",
      "Installing collected packages: exceptiongroup, anyio, httpcore, websockets, strenum, httpx, supafunc, storage3, realtime, postgrest, gotrue, supabase\n",
      "  Attempting uninstall: httpcore\n",
      "    Found existing installation: httpcore 0.12.3\n",
      "    Uninstalling httpcore-0.12.3:\n",
      "      Successfully uninstalled httpcore-0.12.3\n",
      "  Attempting uninstall: websockets\n",
      "    Found existing installation: websockets 9.1\n",
      "    Uninstalling websockets-9.1:\n",
      "      Successfully uninstalled websockets-9.1\n",
      "  Attempting uninstall: httpx\n",
      "    Found existing installation: httpx 0.16.1\n",
      "    Uninstalling httpx-0.16.1:\n",
      "      Successfully uninstalled httpx-0.16.1\n",
      "  Attempting uninstall: gotrue\n",
      "    Found existing installation: gotrue 0.2.0\n",
      "    Uninstalling gotrue-0.2.0:\n",
      "      Successfully uninstalled gotrue-0.2.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "supabase-py 0.0.2 requires gotrue==0.2.0, but you have gotrue 1.3.0 which is incompatible.\n",
      "realtime-py 0.1.3 requires websockets<10.0,>=9.1, but you have websockets 10.4 which is incompatible.\n",
      "postgrest-py 0.4.0 requires httpx<0.17.0,>=0.16.1, but you have httpx 0.24.1 which is incompatible.\u001b[0m\n",
      "Successfully installed anyio-4.0.0 exceptiongroup-1.1.3 gotrue-1.3.0 httpcore-0.17.3 httpx-0.24.1 postgrest-0.13.0 realtime-1.0.0 storage3-0.6.1 strenum-0.4.15 supabase-2.0.3 supafunc-0.3.1 websockets-10.4\n"
     ]
    }
   ],
   "source": [
    "!pip install supabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30f9e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: httpx in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (0.16.1)\r\n",
      "Requirement already satisfied: certifi in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx) (2022.6.15)\r\n",
      "Requirement already satisfied: httpcore==0.12.* in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx) (0.12.3)\r\n",
      "Requirement already satisfied: sniffio in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx) (1.3.0)\r\n",
      "Requirement already satisfied: rfc3986[idna2008]<2,>=1.3 in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpx) (1.5.0)\r\n",
      "Requirement already satisfied: h11==0.* in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from httpcore==0.12.*->httpx) (0.14.0)\r\n",
      "Requirement already satisfied: idna in /Users/nikitayeole/anaconda3/envs/gc_env/lib/python3.10/site-packages (from rfc3986[idna2008]<2,>=1.3->httpx) (2.10)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install httpx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c9060419",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 15:53:06,584:INFO - HTTP Request: GET https://tmwcifbhcnkaiqsldmlz.supabase.co/rest/v1/table_recent?select=vector_data \"HTTP/1.1 200 OK\"\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from supabase import create_client, Client\n",
    "\n",
    "# Supabase details\n",
    "url: str = \"https://tmwcifbhcnkaiqsldmlz.supabase.co\"\n",
    "key: str = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRtd2NpZmJoY25rYWlxc2xkbWx6Iiwicm9sZSI6ImFub24iLCJpYXQiOjE2OTkwNDI0MzAsImV4cCI6MjAxNDYxODQzMH0.tyy9IRYaXs7wkztSFAPUPSKGVz7Ng20xb_QCfWrObws\"\n",
    "\n",
    "# Create a client to connect to Supabase\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# Query to get vectors from your table (modify table name and column as per your schema)\n",
    "data = supabase.table(\"table_recent\").select(\"vector_data\").execute()\n",
    "\n",
    "# Assuming vectors are stored as arrays and not serialized\n",
    "vectors = [row['vector_data'] for row in data.data]\n",
    "\n",
    "if isinstance(vectors[0], str):\n",
    "    vectors = [json.loads(v) for v in vectors]\n",
    "\n",
    "# Convert the list of vectors to a 2D numpy array\n",
    "vectors = np.array(vectors)\n",
    "\n",
    "# Now 'vectors' contains the vectors from your table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e09c4eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04616312e-02, -1.19582536e-02, -1.97615083e-02, ...,\n",
       "        -4.11469443e-03,  1.08264564e-02, -1.60051417e-02],\n",
       "       [ 6.85736611e-02, -2.09120974e-01,  9.37654078e-02, ...,\n",
       "        -5.29735863e-01,  5.10803163e-01, -1.93054885e-01],\n",
       "       [ 3.96974981e-02, -6.17106199e-01, -2.03737959e-01, ...,\n",
       "        -3.06010187e-01,  4.10984129e-01,  1.84191912e-01],\n",
       "       ...,\n",
       "       [ 7.29532719e-01, -1.74154842e+00, -1.74029219e+00, ...,\n",
       "         5.70818126e-01,  1.49535263e+00, -2.34248257e+00],\n",
       "       [-1.36042908e-02, -1.35476971e+00, -1.94295430e+00, ...,\n",
       "        -8.11138749e-02, -1.22766316e+00, -2.89515138e+00],\n",
       "       [ 2.85035276e+00, -5.68446636e+00, -1.41683304e+00, ...,\n",
       "        -2.30134940e+00, -8.10635149e-01, -1.83036995e+00]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bced2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e857d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    # Assuming you've downloaded the required nltk packages such as 'punkt', 'stopwords'\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Tokenize the sentence\n",
    "    tokens = word_tokenize(sentence.lower())\n",
    "    # Remove non-alphabetic characters and stopwords\n",
    "    tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9bef942",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/nikitayeole/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nikitayeole/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK resources, if not already downloaded\n",
    "def download_nltk_resources():\n",
    "    nltk.download('punkt')      # For tokenization\n",
    "    nltk.download('stopwords')  # For removing stop words\n",
    "\n",
    "# Ensure NLTK resources are downloaded before proceeding\n",
    "download_nltk_resources()\n",
    "\n",
    "# ... rest of your code ...\n",
    "\n",
    "\n",
    "def extract_text_by_page(pdf_path):\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for i, page in enumerate(reader.pages):\n",
    "            # Extract text from page and preprocess\n",
    "            text = page.extract_text()\n",
    "            sentences = nltk.sent_tokenize(text)\n",
    "            # Preprocess each sentence and combine into one list of words for the page\n",
    "            words = [preprocess_sentence(sentence) for sentence in sentences]\n",
    "            words = [word for sublist in words for word in sublist]  # Flatten the list\n",
    "            yield TaggedDocument(words=words, tags=[str(i)])\n",
    "pdf_path = 'AI-book.pdf'\n",
    "tagged_data = list(extract_text_by_page(pdf_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0686642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 15:03:35,789:INFO - Doc2Vec lifecycle event {'params': 'Doc2Vec<dm/m,d50,n5,w5,mc5,s0.001,t4>', 'datetime': '2023-11-10T15:03:35.789578', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'created'}\n",
      "2023-11-10 15:03:35,791:INFO - collecting all words and their counts\n",
      "2023-11-10 15:03:35,792:INFO - PROGRESS: at example #0, processed 0 words (0 words/s), 0 word types, 0 tags\n",
      "2023-11-10 15:03:35,864:INFO - collected 28892 word types and 1153 unique tags from a corpus of 1153 examples and 281697 words\n",
      "2023-11-10 15:03:35,864:INFO - Creating a fresh vocabulary\n",
      "2023-11-10 15:03:35,879:INFO - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 retains 6246 unique words (21.62% of original 28892, drops 22646)', 'datetime': '2023-11-10T15:03:35.879237', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-10 15:03:35,879:INFO - Doc2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 248011 word corpus (88.04% of original 281697, drops 33686)', 'datetime': '2023-11-10T15:03:35.879574', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-10 15:03:35,892:INFO - deleting the raw counts dictionary of 28892 items\n",
      "2023-11-10 15:03:35,901:INFO - sample=0.001 downsamples 32 most-common words\n",
      "2023-11-10 15:03:35,902:INFO - Doc2Vec lifecycle event {'msg': 'downsampling leaves estimated 238749.98766387967 word corpus (96.3%% of prior 248011)', 'datetime': '2023-11-10T15:03:35.902157', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'prepare_vocab'}\n",
      "2023-11-10 15:03:35,924:INFO - estimated required memory for 6246 words and 50 dimensions: 6082600 bytes\n",
      "2023-11-10 15:03:35,924:INFO - resetting layer weights\n",
      "2023-11-10 15:03:35,927:INFO - Doc2Vec lifecycle event {'msg': 'training model with 4 workers on 6246 vocabulary and 50 features, using sg=0 hs=0 sample=0.001 negative=5 window=5 shrink_windows=True', 'datetime': '2023-11-10T15:03:35.927515', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-11-10 15:03:35,993:INFO - EPOCH 0: training on 281697 raw words (239764 effective words) took 0.1s, 3679763 effective words/s\n",
      "2023-11-10 15:03:36,058:INFO - EPOCH 1: training on 281697 raw words (239868 effective words) took 0.1s, 3777387 effective words/s\n",
      "2023-11-10 15:03:36,121:INFO - EPOCH 2: training on 281697 raw words (240052 effective words) took 0.1s, 3885028 effective words/s\n",
      "2023-11-10 15:03:36,184:INFO - EPOCH 3: training on 281697 raw words (239795 effective words) took 0.1s, 3839543 effective words/s\n",
      "2023-11-10 15:03:36,247:INFO - EPOCH 4: training on 281697 raw words (239955 effective words) took 0.1s, 3849463 effective words/s\n",
      "2023-11-10 15:03:36,310:INFO - EPOCH 5: training on 281697 raw words (239833 effective words) took 0.1s, 3862872 effective words/s\n",
      "2023-11-10 15:03:36,374:INFO - EPOCH 6: training on 281697 raw words (239946 effective words) took 0.1s, 3844642 effective words/s\n",
      "2023-11-10 15:03:36,437:INFO - EPOCH 7: training on 281697 raw words (239881 effective words) took 0.1s, 3854085 effective words/s\n",
      "2023-11-10 15:03:36,502:INFO - EPOCH 8: training on 281697 raw words (240025 effective words) took 0.1s, 3768892 effective words/s\n",
      "2023-11-10 15:03:36,569:INFO - EPOCH 9: training on 281697 raw words (239960 effective words) took 0.1s, 3640296 effective words/s\n",
      "2023-11-10 15:03:36,632:INFO - EPOCH 10: training on 281697 raw words (239982 effective words) took 0.1s, 3887167 effective words/s\n",
      "2023-11-10 15:03:36,695:INFO - EPOCH 11: training on 281697 raw words (239919 effective words) took 0.1s, 3858876 effective words/s\n",
      "2023-11-10 15:03:36,759:INFO - EPOCH 12: training on 281697 raw words (239991 effective words) took 0.1s, 3810187 effective words/s\n",
      "2023-11-10 15:03:36,823:INFO - EPOCH 13: training on 281697 raw words (239985 effective words) took 0.1s, 3848558 effective words/s\n",
      "2023-11-10 15:03:36,887:INFO - EPOCH 14: training on 281697 raw words (239865 effective words) took 0.1s, 3810196 effective words/s\n",
      "2023-11-10 15:03:36,950:INFO - EPOCH 15: training on 281697 raw words (239776 effective words) took 0.1s, 3838133 effective words/s\n",
      "2023-11-10 15:03:37,014:INFO - EPOCH 16: training on 281697 raw words (239952 effective words) took 0.1s, 3832750 effective words/s\n",
      "2023-11-10 15:03:37,084:INFO - EPOCH 17: training on 281697 raw words (239891 effective words) took 0.1s, 3466165 effective words/s\n",
      "2023-11-10 15:03:37,165:INFO - EPOCH 18: training on 281697 raw words (240004 effective words) took 0.1s, 2981787 effective words/s\n",
      "2023-11-10 15:03:37,233:INFO - EPOCH 19: training on 281697 raw words (239984 effective words) took 0.1s, 3573182 effective words/s\n",
      "2023-11-10 15:03:37,302:INFO - EPOCH 20: training on 281697 raw words (239830 effective words) took 0.1s, 3595430 effective words/s\n",
      "2023-11-10 15:03:37,367:INFO - EPOCH 21: training on 281697 raw words (239855 effective words) took 0.1s, 3721755 effective words/s\n",
      "2023-11-10 15:03:37,438:INFO - EPOCH 22: training on 281697 raw words (239871 effective words) took 0.1s, 3436818 effective words/s\n",
      "2023-11-10 15:03:37,507:INFO - EPOCH 23: training on 281697 raw words (239964 effective words) took 0.1s, 3533544 effective words/s\n",
      "2023-11-10 15:03:37,572:INFO - EPOCH 24: training on 281697 raw words (239878 effective words) took 0.1s, 3786601 effective words/s\n",
      "2023-11-10 15:03:37,635:INFO - EPOCH 25: training on 281697 raw words (239907 effective words) took 0.1s, 3879158 effective words/s\n",
      "2023-11-10 15:03:37,698:INFO - EPOCH 26: training on 281697 raw words (239924 effective words) took 0.1s, 3869511 effective words/s\n",
      "2023-11-10 15:03:37,762:INFO - EPOCH 27: training on 281697 raw words (239877 effective words) took 0.1s, 3837354 effective words/s\n",
      "2023-11-10 15:03:37,825:INFO - EPOCH 28: training on 281697 raw words (239825 effective words) took 0.1s, 3850737 effective words/s\n",
      "2023-11-10 15:03:37,889:INFO - EPOCH 29: training on 281697 raw words (239833 effective words) took 0.1s, 3838300 effective words/s\n",
      "2023-11-10 15:03:37,952:INFO - EPOCH 30: training on 281697 raw words (239890 effective words) took 0.1s, 3824670 effective words/s\n",
      "2023-11-10 15:03:38,015:INFO - EPOCH 31: training on 281697 raw words (239841 effective words) took 0.1s, 3859962 effective words/s\n",
      "2023-11-10 15:03:38,079:INFO - EPOCH 32: training on 281697 raw words (239964 effective words) took 0.1s, 3850014 effective words/s\n",
      "2023-11-10 15:03:38,145:INFO - EPOCH 33: training on 281697 raw words (239950 effective words) took 0.1s, 3686337 effective words/s\n",
      "2023-11-10 15:03:38,210:INFO - EPOCH 34: training on 281697 raw words (239949 effective words) took 0.1s, 3772309 effective words/s\n",
      "2023-11-10 15:03:38,276:INFO - EPOCH 35: training on 281697 raw words (239967 effective words) took 0.1s, 3678705 effective words/s\n",
      "2023-11-10 15:03:38,343:INFO - EPOCH 36: training on 281697 raw words (239955 effective words) took 0.1s, 3613671 effective words/s\n",
      "2023-11-10 15:03:38,409:INFO - EPOCH 37: training on 281697 raw words (239909 effective words) took 0.1s, 3729733 effective words/s\n",
      "2023-11-10 15:03:38,473:INFO - EPOCH 38: training on 281697 raw words (240075 effective words) took 0.1s, 3764576 effective words/s\n",
      "2023-11-10 15:03:38,541:INFO - EPOCH 39: training on 281697 raw words (239979 effective words) took 0.1s, 3580224 effective words/s\n",
      "2023-11-10 15:03:38,613:INFO - EPOCH 40: training on 281697 raw words (240100 effective words) took 0.1s, 3437171 effective words/s\n",
      "2023-11-10 15:03:38,678:INFO - EPOCH 41: training on 281697 raw words (239962 effective words) took 0.1s, 3720879 effective words/s\n",
      "2023-11-10 15:03:38,746:INFO - EPOCH 42: training on 281697 raw words (239896 effective words) took 0.1s, 3614949 effective words/s\n",
      "2023-11-10 15:03:38,812:INFO - EPOCH 43: training on 281697 raw words (239851 effective words) took 0.1s, 3671699 effective words/s\n",
      "2023-11-10 15:03:38,876:INFO - EPOCH 44: training on 281697 raw words (239999 effective words) took 0.1s, 3823528 effective words/s\n",
      "2023-11-10 15:03:38,940:INFO - EPOCH 45: training on 281697 raw words (239851 effective words) took 0.1s, 3849895 effective words/s\n",
      "2023-11-10 15:03:39,006:INFO - EPOCH 46: training on 281697 raw words (239923 effective words) took 0.1s, 3643100 effective words/s\n",
      "2023-11-10 15:03:39,072:INFO - EPOCH 47: training on 281697 raw words (240060 effective words) took 0.1s, 3739798 effective words/s\n",
      "2023-11-10 15:03:39,136:INFO - EPOCH 48: training on 281697 raw words (239885 effective words) took 0.1s, 3811573 effective words/s\n",
      "2023-11-10 15:03:39,199:INFO - EPOCH 49: training on 281697 raw words (239910 effective words) took 0.1s, 3827000 effective words/s\n",
      "2023-11-10 15:03:39,263:INFO - EPOCH 50: training on 281697 raw words (239850 effective words) took 0.1s, 3834601 effective words/s\n",
      "2023-11-10 15:03:39,327:INFO - EPOCH 51: training on 281697 raw words (239938 effective words) took 0.1s, 3817002 effective words/s\n",
      "2023-11-10 15:03:39,391:INFO - EPOCH 52: training on 281697 raw words (239803 effective words) took 0.1s, 3817101 effective words/s\n",
      "2023-11-10 15:03:39,454:INFO - EPOCH 53: training on 281697 raw words (239946 effective words) took 0.1s, 3867194 effective words/s\n",
      "2023-11-10 15:03:39,517:INFO - EPOCH 54: training on 281697 raw words (239802 effective words) took 0.1s, 3857128 effective words/s\n",
      "2023-11-10 15:03:39,596:INFO - EPOCH 55: training on 281697 raw words (239967 effective words) took 0.1s, 3073535 effective words/s\n",
      "2023-11-10 15:03:39,662:INFO - EPOCH 56: training on 281697 raw words (239927 effective words) took 0.1s, 3701099 effective words/s\n",
      "2023-11-10 15:03:39,728:INFO - EPOCH 57: training on 281697 raw words (239750 effective words) took 0.1s, 3704510 effective words/s\n",
      "2023-11-10 15:03:39,792:INFO - EPOCH 58: training on 281697 raw words (239861 effective words) took 0.1s, 3797154 effective words/s\n",
      "2023-11-10 15:03:39,858:INFO - EPOCH 59: training on 281697 raw words (239968 effective words) took 0.1s, 3718366 effective words/s\n",
      "2023-11-10 15:03:39,924:INFO - EPOCH 60: training on 281697 raw words (239715 effective words) took 0.1s, 3648961 effective words/s\n",
      "2023-11-10 15:03:39,988:INFO - EPOCH 61: training on 281697 raw words (239870 effective words) took 0.1s, 3820777 effective words/s\n",
      "2023-11-10 15:03:40,058:INFO - EPOCH 62: training on 281697 raw words (239892 effective words) took 0.1s, 3489399 effective words/s\n",
      "2023-11-10 15:03:40,127:INFO - EPOCH 63: training on 281697 raw words (239802 effective words) took 0.1s, 3581939 effective words/s\n",
      "2023-11-10 15:03:40,198:INFO - EPOCH 64: training on 281697 raw words (239800 effective words) took 0.1s, 3414256 effective words/s\n",
      "2023-11-10 15:03:40,265:INFO - EPOCH 65: training on 281697 raw words (239886 effective words) took 0.1s, 3627110 effective words/s\n",
      "2023-11-10 15:03:40,331:INFO - EPOCH 66: training on 281697 raw words (239972 effective words) took 0.1s, 3729763 effective words/s\n",
      "2023-11-10 15:03:40,395:INFO - EPOCH 67: training on 281697 raw words (239969 effective words) took 0.1s, 3812988 effective words/s\n",
      "2023-11-10 15:03:40,458:INFO - EPOCH 68: training on 281697 raw words (239963 effective words) took 0.1s, 3840394 effective words/s\n",
      "2023-11-10 15:03:40,521:INFO - EPOCH 69: training on 281697 raw words (239883 effective words) took 0.1s, 3859282 effective words/s\n",
      "2023-11-10 15:03:40,584:INFO - EPOCH 70: training on 281697 raw words (240020 effective words) took 0.1s, 3849971 effective words/s\n",
      "2023-11-10 15:03:40,649:INFO - EPOCH 71: training on 281697 raw words (239847 effective words) took 0.1s, 3791153 effective words/s\n",
      "2023-11-10 15:03:40,712:INFO - EPOCH 72: training on 281697 raw words (239946 effective words) took 0.1s, 3842020 effective words/s\n",
      "2023-11-10 15:03:40,776:INFO - EPOCH 73: training on 281697 raw words (239825 effective words) took 0.1s, 3836108 effective words/s\n",
      "2023-11-10 15:03:40,840:INFO - EPOCH 74: training on 281697 raw words (240065 effective words) took 0.1s, 3825585 effective words/s\n",
      "2023-11-10 15:03:40,903:INFO - EPOCH 75: training on 281697 raw words (239852 effective words) took 0.1s, 3831670 effective words/s\n",
      "2023-11-10 15:03:40,966:INFO - EPOCH 76: training on 281697 raw words (239903 effective words) took 0.1s, 3847536 effective words/s\n",
      "2023-11-10 15:03:41,029:INFO - EPOCH 77: training on 281697 raw words (240034 effective words) took 0.1s, 3884666 effective words/s\n",
      "2023-11-10 15:03:41,093:INFO - EPOCH 78: training on 281697 raw words (239929 effective words) took 0.1s, 3824796 effective words/s\n",
      "2023-11-10 15:03:41,157:INFO - EPOCH 79: training on 281697 raw words (239825 effective words) took 0.1s, 3809911 effective words/s\n",
      "2023-11-10 15:03:41,219:INFO - EPOCH 80: training on 281697 raw words (239954 effective words) took 0.1s, 3873527 effective words/s\n",
      "2023-11-10 15:03:41,282:INFO - EPOCH 81: training on 281697 raw words (239910 effective words) took 0.1s, 3859372 effective words/s\n",
      "2023-11-10 15:03:41,346:INFO - EPOCH 82: training on 281697 raw words (239819 effective words) took 0.1s, 3849801 effective words/s\n",
      "2023-11-10 15:03:41,409:INFO - EPOCH 83: training on 281697 raw words (239980 effective words) took 0.1s, 3866815 effective words/s\n",
      "2023-11-10 15:03:41,472:INFO - EPOCH 84: training on 281697 raw words (239869 effective words) took 0.1s, 3850426 effective words/s\n",
      "2023-11-10 15:03:41,535:INFO - EPOCH 85: training on 281697 raw words (239926 effective words) took 0.1s, 3852804 effective words/s\n",
      "2023-11-10 15:03:41,598:INFO - EPOCH 86: training on 281697 raw words (239922 effective words) took 0.1s, 3856828 effective words/s\n",
      "2023-11-10 15:03:41,661:INFO - EPOCH 87: training on 281697 raw words (239914 effective words) took 0.1s, 3871264 effective words/s\n",
      "2023-11-10 15:03:41,724:INFO - EPOCH 88: training on 281697 raw words (240000 effective words) took 0.1s, 3876994 effective words/s\n",
      "2023-11-10 15:03:41,787:INFO - EPOCH 89: training on 281697 raw words (239973 effective words) took 0.1s, 3820207 effective words/s\n",
      "2023-11-10 15:03:41,852:INFO - EPOCH 90: training on 281697 raw words (239725 effective words) took 0.1s, 3751499 effective words/s\n",
      "2023-11-10 15:03:41,927:INFO - EPOCH 91: training on 281697 raw words (239936 effective words) took 0.1s, 3262684 effective words/s\n",
      "2023-11-10 15:03:41,993:INFO - EPOCH 92: training on 281697 raw words (239871 effective words) took 0.1s, 3736871 effective words/s\n",
      "2023-11-10 15:03:42,057:INFO - EPOCH 93: training on 281697 raw words (239982 effective words) took 0.1s, 3796195 effective words/s\n",
      "2023-11-10 15:03:42,123:INFO - EPOCH 94: training on 281697 raw words (239775 effective words) took 0.1s, 3674032 effective words/s\n",
      "2023-11-10 15:03:42,186:INFO - EPOCH 95: training on 281697 raw words (239775 effective words) took 0.1s, 3868891 effective words/s\n",
      "2023-11-10 15:03:42,251:INFO - EPOCH 96: training on 281697 raw words (239980 effective words) took 0.1s, 3788632 effective words/s\n",
      "2023-11-10 15:03:42,314:INFO - EPOCH 97: training on 281697 raw words (239912 effective words) took 0.1s, 3848418 effective words/s\n",
      "2023-11-10 15:03:42,378:INFO - EPOCH 98: training on 281697 raw words (240007 effective words) took 0.1s, 3828909 effective words/s\n",
      "2023-11-10 15:03:42,442:INFO - EPOCH 99: training on 281697 raw words (239992 effective words) took 0.1s, 3788039 effective words/s\n",
      "2023-11-10 15:03:42,443:INFO - Doc2Vec lifecycle event {'msg': 'training on 28169700 raw words (23991140 effective words) took 6.5s, 3682346 effective words/s', 'datetime': '2023-11-10T15:03:42.443069', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'train'}\n",
      "2023-11-10 15:03:42,443:INFO - Doc2Vec lifecycle event {'fname_or_handle': 'jesc102_model.d2v', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2023-11-10T15:03:42.443370', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'saving'}\n",
      "2023-11-10 15:03:42,443:INFO - not storing attribute cum_table\n",
      "2023-11-10 15:03:42,450:INFO - saved jesc102_model.d2v\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(vector_size=50, window=5, min_count=5, workers=4, epochs=100)\n",
    "model.build_vocab(tagged_data)\n",
    "model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"jesc102_model.d2v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77297d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-10 15:03:58,288:INFO - loading Doc2Vec object from jesc102_model.d2v\n",
      "2023-11-10 15:03:58,298:INFO - loading dv recursively from jesc102_model.d2v.dv.* with mmap=None\n",
      "2023-11-10 15:03:58,299:INFO - loading wv recursively from jesc102_model.d2v.wv.* with mmap=None\n",
      "2023-11-10 15:03:58,299:INFO - setting ignored attribute cum_table to None\n",
      "2023-11-10 15:03:58,343:INFO - Doc2Vec lifecycle event {'fname': 'jesc102_model.d2v', 'datetime': '2023-11-10T15:03:58.343780', 'gensim': '4.3.2', 'python': '3.10.4 (main, Mar 31 2022, 03:37:37) [Clang 12.0.0 ]', 'platform': 'macOS-12.5.1-arm64-arm-64bit', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = Doc2Vec.load(\"jesc102_model.d2v\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "273d86aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.96717757 -0.6468551  -1.329459    0.9977114   3.2176983  -1.001382\n",
      " -1.2468129   0.9271196   1.6404973  -0.345029   -0.5411573   2.195142\n",
      " -0.5824521  -2.5141656  -0.20762305 -1.3571634   0.5982128   2.3713372\n",
      " -1.2842461  -2.3825943   0.83894473  2.252717    2.5290744  -0.80416596\n",
      "  1.537438    2.081704    0.6721183   0.4353411  -0.5737098   1.8207092\n",
      " -2.6777172   3.6231334   0.939394    0.79166    -0.8216593  -3.1278615\n",
      " -1.3754977  -2.3014343  -0.04148308 -2.9555888   0.03748488  1.423045\n",
      "  2.3721616  -0.27408355  2.8329854  -0.95357794  1.0332091  -1.8246237\n",
      "  3.1473517   1.1078045 ]\n"
     ]
    }
   ],
   "source": [
    "# Infer a vector for a new sentence\n",
    "new_sentence = \"\"\"Although efficient special-purpose algorithms exist for this problem and for the whole\n",
    "n-queens family, it remains a useful test problem for search algorithms. There are two main\n",
    "kinds of formulation. An incremental INCREMENTAL formulation involves operators that augment the state\n",
    "FORMULATION\n",
    "description, starting with an empty state; for the 8-queens problem, this means that each\n",
    "COMPLETE-STATE action adds a queen to the state. A complete-state formulation starts with all 8 queens on\n",
    "FORMULATION\n",
    "the board and moves them around. In either case, the path cost is of no interest because only\n",
    "the final state counts. The first incremental formulation one might try is the following:\"\"\"\n",
    "vector_sentence = model.infer_vector(preprocess_sentence(new_sentence))\n",
    "print(vector_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2925ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "969e45f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04616312e-02, -1.19582536e-02, -1.97615083e-02, ...,\n",
       "        -4.11469443e-03,  1.08264564e-02, -1.60051417e-02],\n",
       "       [ 6.85736611e-02, -2.09120974e-01,  9.37654078e-02, ...,\n",
       "        -5.29735863e-01,  5.10803163e-01, -1.93054885e-01],\n",
       "       [ 3.96974981e-02, -6.17106199e-01, -2.03737959e-01, ...,\n",
       "        -3.06010187e-01,  4.10984129e-01,  1.84191912e-01],\n",
       "       ...,\n",
       "       [ 7.29532719e-01, -1.74154842e+00, -1.74029219e+00, ...,\n",
       "         5.70818126e-01,  1.49535263e+00, -2.34248257e+00],\n",
       "       [-1.36042908e-02, -1.35476971e+00, -1.94295430e+00, ...,\n",
       "        -8.11138749e-02, -1.22766316e+00, -2.89515138e+00],\n",
       "       [ 2.85035276e+00, -5.68446636e+00, -1.41683304e+00, ...,\n",
       "        -2.30134940e+00, -8.10635149e-01, -1.83036995e+00]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9465620f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "186a158c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49e3a35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c3cbced1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91\n"
     ]
    }
   ],
   "source": [
    "vector_sentence = np.array(vector_sentence)\n",
    "vector_sentence_2d = vector_sentence.reshape(1, -1)\n",
    "\n",
    "similarities = cosine_similarity(vector_sentence_2d, vectors)\n",
    "\n",
    "# Find the index of the most similar paragraph\n",
    "most_similar_index = np.argmax(similarities)\n",
    "print(most_similar_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e44932e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f65b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bibliographical and Historical Notes 109\n",
    "–A∗search expands nodes with minimal f(n)=g(n)+h(n).A∗is complete and\n",
    "optimal, provided that h(n)is admissible (for T REE-SEARCH ) or consistent (for\n",
    "GRAPH -SEARCH ). The space complexity of A∗is still prohibitive.\n",
    "–R B F S (recursive best-ﬁrst search) and SMA∗(simpliﬁed memory-bounded A∗)\n",
    "are robust, optimal search algorithms that use limited amounts of memory; givenenough time, they can solve problems that A\n",
    "∗cannot solve because it runs out of\n",
    "memory.\n",
    "•The performance of heuristic search algorithms depends on the quality of the heuristicfunction. One can sometimes construct good heuristics by relaxing the problem deﬁ-nition, by storing precomputed solution costs for subproblems in a pattern database, orby learning from experience with the problem class.\n",
    "BIBLIOGRAPHICAL AND HISTORICAL NOTES\n",
    "The topic of state-space search originated in more or less its current form in the early years ofAI. Newell and Simon’s work on the Logic Theorist (1957) and GPS (1961) led to the estab-lishment of search algorithms as the primary weapons in the armory of 1960s AI researchersand to the establishment of problem solving as the canonical AI task. Work in operationsresearch by Richard Bellman (1957) showed the importance of additive path costs in sim-plifying optimization algorithms. The text on Automated Problem Solving by Nils Nilsson\n",
    "(1971) established the area on a solid theoretical footing.\n",
    "Most of the state-space search problems analyzed in this chapter have a long history\n",
    "in the literature and are less trivial than they might seem. The missionaries and cannibalsproblem used in Exercise 3.9 was analyzed in detail by Amarel (1968). It had been consid-ered earlier—in AI by Simon and Newell (1961) and in operations research by Bellman andDreyfus (1962).\n",
    "The 8-puzzle is a smaller cousin of the 15-puzzle, whose history is recounted at length\n",
    "by Slocum and Sonneveld (2006). It was widely believed to have been invented by the fa-mous American game designer Sam Loyd, based on his claims to that effect from 1891 on-ward (Loyd, 1959). Actually it was invented by Noyes Chapman, a postmaster in Canastota,New York, in the mid-1870s. (Chapman was unable to patent his invention, as a genericpatent covering sliding blocks with letters, numbers, or pictures was granted to Ernest Kinseyin 1878.) It quickly attracted the attention of the public and of mathematicians (Johnson andStory, 1879; Tait, 1880). The editors of the American Journal of Mathematics stated, “The\n",
    "‘15’ puzzle for the last few weeks has been prominently before the American public, and maysafely be said to have engaged the attention of nine out of ten persons of both sexes and allages and conditions of the community.” Ratner and Warmuth (1986) showed that the generaln×nversion of the 15-puzzle belongs to the class of NP-complete problems.\n",
    "The 8-queens problem was ﬁrst published anonymously in the German chess maga-\n",
    "zine Schach in 1848; it was later attributed to one Max Bezzel. It was republished in 1850\n",
    "and at that time drew the attention of the eminent mathematician Carl Friedrich Gauss, who"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
